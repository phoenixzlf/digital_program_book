DATE,SESSION,TYPE_START,FROM,TO,LOCATION,SESSION_NUM,TYPE,TOPIC,CHAIR,CHAIR_CL,CHAIR_CONFIRMED,SPEAKER,TITLE,ABSTRACT,ABSTRACT_CL,REFERENCES,SPEAKER1,SPEAKER2,AFF1,AFF2,AUTHORS,TOPIC_REASSIGNED
7/30/2023,R,5:00 PM,5:00 PM,8:00 PM,Olmsted Hall,NA,Reception,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
7/31/2023,A,8:30 AM,8:30 AM,9:00 AM,Aliber Hall,NA,Breakfast,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
7/31/2023,A,9:00 AM,9:00 AM,9:10 AM,Aliber 101,1,Welcome,NA,NA,NA,NA,Alejandro Hernandez,NA,NA,NA,NA,Alejandro Hernandez,NA,"Zimpleman College of Business, Drake University",NA,NA,NA
7/31/2023,A,9:10 AM,9:10 AM,9:20 AM,Aliber 101,1,Opening Remarks,NA,NA,NA,NA,Larry Zimpleman,NA,NA,NA,NA,Larry Zimpleman,NA,NA,NA,NA,NA
7/31/2023,A,9:20 AM,9:20 AM,10:20 AM,Aliber 101,1,Keynote Session,NA,Elias Shiu,Elias Shiu,TRUE,Montserrat Guillen,Pricing Motor Insurance With Telematics Data,"Many insurance companies collect telematics data about drivers’ exposure to traffic (distance driven and type of road), their driving behavior (excess speed, aggressiveness, operating hours) and contextual information (weather conditions, traffic intensity). Actuaries can use this information to improve motor insurance ratemaking. Several recent proposals will be presented, mostly using traditional GLM models and addressing weekly ratemaking. Personalized driving risk indicators can also promote driving safety. Illustrations with several real data sets provided by insurance companies will answer questions: (1) How are pay-per-mile insurance schemes be designed? (2) How can near-miss telematics be used to identify risky drivers? (3) Can external factors such as weather conditions be integrated?","Many insurance companies collect telematics data about drivers’ exposure to traffic (distance driven and type of road), their driving behavior (excess speed, aggressiveness, operating hours) and contextual information (weather conditions, traffic intensity). Actuaries can use this information to improve motor insurance ratemaking. Several recent proposals will be presented, mostly using traditional GLM models and addressing weekly ratemaking. Personalized driving risk indicators can also promote driving safety. Illustrations with several real data sets provided by insurance companies will answer questions: (1) How are pay-per-mile insurance schemes be designed? (2) How can near-miss telematics be used to identify risky drivers? (3) Can external factors such as weather conditions be integrated?",,Montserrat Guillen,NA,University of Barcelona,NA,NA,NA
7/31/2023,A,10:20 AM,10:20 AM,10:50 AM,Aliber Hall,NA,Coffee Break,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
7/31/2023,A,10:50 AM,10:50 AM,11:20 AM,Aliber 101,2,Parallel Session,(Invited Session) Climate Change: Models and Data,NA,NA,NA,Steve Jackson and Peter Ott,The More Things Change: NOAA Weather Station and ERA5 Reanalysis Climate Data Compared,"Assessing changes in climate requires a careful weighing of several options which exist for sources of data.  As part of its contribution to the revision of the Actuaries Climate Index, we are examining several elements from the ERA5 reanalysis database produced by the European Centre for Midrange Weather Forecasts, comparing them to observational sources (the Global Historical Climate Network, and the Permanent Service for Mean Sea Level).  In this first paper in a series of assessments we examine temperature and precipitation, and evaluate three dimensions: 1) the coverage and completeness of that coverage; 2) the consistency of results from the two sources; and 3) the performance of the two sources in special circumstances (such as Hurricane Katrina).  In general, we find ERA5 to be far superior in completeness and coverage, roughly consistent with observational data, and equal or superior to the observational data in special circumstances.  In the latter two findings we also note an important caveat: while ERA5 allows us to perform much more granular analysis than observational records, there is some limit to the granularity which produces results consistent with the observational.  The implications of these results for a use such as that of the Actuaries Climate Index are then discussed.","Assessing changes in climate requires a careful weighing of several options which exist for sources of data. As part of its contribution to the revision of the Actuaries Climate Index, we are examining several elements from the ERA5 reanalysis database produced by the European Centre for Midrange Weather Forecasts, comparing them to observational sources (the Global Historical Climate Network, and the Permanent Service for Mean Sea Level). In this first paper in a series of assessments we examine temperature and precipitation, and evaluate three dimensions: 1) the coverage and completeness of that coverage; 2) the consistency of results from the two sources; and 3) the performance of the two sources in special circumstances (such as Hurricane Katrina). In general, we find ERA5 to be far superior in completeness and coverage, roughly consistent with observational data, and equal or superior to the observational data in special circumstances. In the latter two findings we also note an important caveat: while ERA5 allows us to perform much more granular analysis than observational records, there is some limit to the granularity which produces results consistent with the observational. The implications of these results for a use such as that of the Actuaries Climate Index are then discussed.",,Steve Jackson and Peter Ott, ,American Academy of Actuaries,,,Climate Change: Models and Data
7/31/2023,A,10:50 AM,11:20 AM,11:50 AM,Aliber 101,2,Parallel Session,(Invited Session) Climate Change: Models and Data,NA,NA,NA,"Chia-Ying Lee, Adam H. Sobel, Michael K. Tippett, Suzana J. Camargo, Marc Wuest, Michael Wehner, and Hiroyuki Murakami",Climate Change Signal in Atlantic Tropical Cyclones Today and Near Future,"We use a set of computer model simulations to study recent trends in Atlantic hurricanes. We looked at three aspects of these storms: the number of tropical cyclones each year, which has fluctuated up and down over time (but generally increased over the last several decades); the strength of their winds, which has been increasing; and the speed at which they move, which has been decreasing. These trends could be caused either by human-induced global warming or by natural variability; determining which cause is more important to overall hurricane risk requires us to understand how the number of tropical cyclones per year responds to warming. In our simulations, this number can either increase or decrease with warming, depending on which of two nearly identical versions of our model we use to simulate the storms. This uncertainty prevents us from reaching definitive conclusions about either present or future hurricane risk. Nonetheless, our analysis suggests that the risk of Atlantic hurricanes is more likely increasing than decreasing, and we argue that from a broader point of view, this is effectively equivalent to saying the risk is increasing.","We use a set of computer model simulations to study recent trends in Atlantic hurricanes. We looked at three aspects of these storms: the number of tropical cyclones each year, which has fluctuated up and down over time (but generally increased over the last several decades); the strength of their winds, which has been increasing; and the speed at which they move, which has been decreasing. These trends could be caused either by human-induced global warming or by natural variability; determining which cause is more important to overall hurricane risk requires us to understand how the number of tropical cyclones per year responds to warming. In our simulations, this number can either increase or decrease with warming, depending on which of two nearly identical versions of our model we use to simulate the storms. This uncertainty prevents us from reaching definitive conclusions about either present or future hurricane risk. Nonetheless, our analysis suggests that the risk of Atlantic hurricanes is more likely increasing than decreasing, and we argue that from a broader point of view, this is effectively equivalent to saying the risk is increasing.",,"Chia-Ying Lee, Adam H. Sobel, Michael K. Tippett, Suzana J. Camargo, Marc Wuest, Michael Wehner, and Hiroyuki Murakami", ,American Academy of Actuaries,,,Climate Change: Models and Data
7/31/2023,A,10:50 AM,11:50 AM,12:20 PM,Aliber 101,2,Parallel Session,(Invited Session) Climate Change: Models and Data,NA,NA,NA,Lisa Gao and Peng Shi,Risk Modeling of Hail Damage Insurance Claims Using a Factor Copula Regression for Replicated Spatial Data,"The localized nature of hailstorms leads to a concentration of correlated risks that can substantially amplify aggregate storm-level losses. We propose a spatial factor copula to characterize the dependence between hail property damage claims arising from a common storm when analyzing its financial impact. The factor copula captures the spatial dependence among properties that decays with distance, as well as the aspatial dependence induced by the common shock of experiencing the same storm. The framework allows insurers to flexibly incorporate the observed heterogeneity in marginal models of skewed, heavy-tailed, and zero-inflated insurance losses, while retaining the model interpretation in decomposing latent sources of dependence. We present a likelihood-based estimation to address the computational challenges from the discreteness in the outcome and the factor copula in high dimensions. Using replicated spatial hail damage insurance claims data from a U.S. insurer, we demonstrate the effect of dependence on reinsurance and retention decisions.","The localized nature of hailstorms leads to a concentration of correlated risks that can substantially amplify aggregate storm-level losses. We propose a spatial factor copula to characterize the dependence between hail property damage claims arising from a common storm when analyzing its financial impact. The factor copula captures the spatial dependence among properties that decays with distance, as well as the aspatial dependence induced by the common shock of experiencing the same storm. The framework allows insurers to flexibly incorporate the observed heterogeneity in marginal models of skewed, heavy-tailed, and zero-inflated insurance losses, while retaining the model interpretation in decomposing latent sources of dependence. We present a likelihood-based estimation to address the computational challenges from the discreteness in the outcome and the factor copula in high dimensions. Using replicated spatial hail damage insurance claims data from a U.S. insurer, we demonstrate the effect of dependence on reinsurance and retention decisions.",,Lisa Gao and Peng Shi, ,American Academy of Actuaries,,,Climate Change: Models and Data
7/31/2023,A,10:50 AM,10:50 AM,11:20 AM,Aliber 102,3,Parallel Session,Statistical and Machine Learning I,Jinggong Zhang,Jinggong Zhang,TRUE,Danlei Zhu,Unearned Premium Risk and Machine Learning Techniques,"For insurance companies, the unearned premium is the portion of premium that is allocated for the remaining period of a policy or premium that still needs to be earned. Insufficient unearned premium to cover future losses will result in unearned premium risk. Reserves allocated for the unearned premium risk are called premium deficiency reserves (PDRs). Despite their importance, PDRs received less attention from the actuarial community compared to other reserves, and existing researches on PDR mainly focused on utilizing statistical models.  In this presentation, we demonstrate the effectiveness of two popular ensemble machine learning models, XGBoost and Random Forest, in calculating PDR on an extended warranty dataset, which comes under long-duration P&C insurance contracts.  while we include two statistical models as well, the results show that machine learning models predict reserves more accurately. Thus, this research encourages actuaries to consider incorporating machine learning models when calculating PDRs for the unearned premium risk.","For insurance companies, the unearned premium is the portion of premium that is allocated for the remaining period of a policy or premium that still needs to be earned. Insufficient unearned premium to cover future losses will result in unearned premium risk. Reserves allocated for the unearned premium risk are called premium deficiency reserves (PDRs). Despite their importance, PDRs received less attention from the actuarial community compared to other reserves, and existing researches on PDR mainly focused on utilizing statistical models. In this presentation, we demonstrate the effectiveness of two popular ensemble machine learning models, XGBoost and Random Forest, in calculating PDR on an extended warranty dataset, which comes under long-duration P&C insurance contracts. while we include two statistical models as well, the results show that machine learning models predict reserves more accurately. Thus, this research encourages actuaries to consider incorporating machine learning models when calculating PDRs for the unearned premium risk.", ,Danlei Zhu, ,Middle Tennessee State University,,,Statistical and machine learning I
7/31/2023,A,10:50 AM,11:20 AM,11:50 AM,Aliber 102,3,Parallel Session,Statistical and Machine Learning I,Jinggong Zhang,Jinggong Zhang,TRUE,Xing Wang,Multi-Output Extreme Spatial Model for Production Systems," Data-driven spatial models in machine learning have enabled efficient control of production systems. However, most machine learning models are devoted to modeling the mean response, so they are inappropriate to analyze abnormal extreme events that are often the main interests. Since extreme events from tail distribution give rise to prohibitive expenditures in system man- agement, extreme spatial models should be utilized to analyze extreme risks. Recent engineering applications of extreme modeling are limited to simple cases such as univariate modeling, and it is insufficient for complex systems. Moreover, existing extreme spatial models in other domains cannot be directly applied to controllable systems. In this paper, we propose an extreme spatial model that enables the modeling of multi-output response control systems. Robust parameter estimation is proposed for marginal extreme distributions, and efficient composite likelihood estimation is devised to cope with high dimensional problems. The proposed model is applied to the modeling of maximum residual stress in composite aircraft production.","Data-driven spatial models in machine learning have enabled efficient control of production systems. However, most machine learning models are devoted to modeling the mean response, so they are inappropriate to analyze abnormal extreme events that are often the main interests. Since extreme events from tail distribution give rise to prohibitive expenditures in system man- agement, extreme spatial models should be utilized to analyze extreme risks. Recent engineering applications of extreme modeling are limited to simple cases such as univariate modeling, and it is insufficient for complex systems. Moreover, existing extreme spatial models in other domains cannot be directly applied to controllable systems. In this paper, we propose an extreme spatial model that enables the modeling of multi-output response control systems. Robust parameter estimation is proposed for marginal extreme distributions, and efficient composite likelihood estimation is devised to cope with high dimensional problems. The proposed model is applied to the modeling of maximum residual stress in composite aircraft production.", ,Xing Wang, ,Illinois State University,,,Statistical and machine learning III
7/31/2023,A,10:50 AM,11:50 AM,12:20 PM,Aliber 102,3,Parallel Session,Statistical and Machine Learning I,Jinggong Zhang,Jinggong Zhang,TRUE,Jinggong Zhang,Integrated Design for Index Insurance," Weather index insurance (WII) is a promising tool for agricultural risk mitigation, but its popularity is often hindered by challenges of product design, such as basis risk, weather index selection and product complexity issues. In this paper we develop machine learning methodologies to design the statistically optimal WII to address those critical concerns in the literature and practice. The idea from tree-based models is exploited to simultaneously achieve weather variable selection and payout function determination, leading to effective basis risk reduction. The proposed framework is applied to an empirical study where high-dimensional weather variables are adopted to hedge soybean production losses in Iowa. Our numerical results show that the designed insurance policies are potentially viable with much lower government subsidy, and therefore can enhance social welfare.  ","Weather index insurance (WII) is a promising tool for agricultural risk mitigation, but its popularity is often hindered by challenges of product design, such as basis risk, weather index selection and product complexity issues. In this paper we develop machine learning methodologies to design the statistically optimal WII to address those critical concerns in the literature and practice. The idea from tree-based models is exploited to simultaneously achieve weather variable selection and payout function determination, leading to effective basis risk reduction. The proposed framework is applied to an empirical study where high-dimensional weather variables are adopted to hedge soybean production losses in Iowa. Our numerical results show that the designed insurance policies are potentially viable with much lower government subsidy, and therefore can enhance social welfare.", ,Jinggong Zhang, ,Nanyang Technological University,,,Statistical and machine learning I
7/31/2023,A,10:50 AM,10:50 AM,11:20 AM,Aliber 103,4,Parallel Session,Quantitative Finance I,Amin Hassan Zadeh,Amin Hassan Zadeh,TRUE,Zhenzhen Huang,Tail Mean-Variance Portfolio Selection With Estimation Risk," Tail Mean-Variance (TMV) has emerged from the actuarial community as a criterion for risk management and portfolio selection, with a focus on extreme losses. The existing literature on portfolio optimization under the TMV criterion relies on the plug-in approach that substitutes the unknown mean and covariance of asset returns in the optimal portfolio weight with their sample counterparts. The plug-in method inevitably introduces estimation risk and usually has poor out-of-sample performance. We propose an optimal combination of the plug-in and 1/N rules to improve out-of-sample performance. Our proposed combined portfolio consistently outperforms both the plug-in and 1/N portfolios on both simulated and real-world datasets.","Tail Mean-Variance (TMV) has emerged from the actuarial community as a criterion for risk management and portfolio selection, with a focus on extreme losses. The existing literature on portfolio optimization under the TMV criterion relies on the plug-in approach that substitutes the unknown mean and covariance of asset returns in the optimal portfolio weight with their sample counterparts. The plug-in method inevitably introduces estimation risk and usually has poor out-of-sample performance. We propose an optimal combination of the plug-in and 1/N rules to improve out-of-sample performance. Our proposed combined portfolio consistently outperforms both the plug-in and 1/N portfolios on both simulated and real-world datasets.", ,Zhenzhen Huang, ,University of Waterloo,,,Quantitative finance I
7/31/2023,A,10:50 AM,11:20 AM,11:50 AM,Aliber 103,4,Parallel Session,Quantitative Finance I,Amin Hassan Zadeh,Amin Hassan Zadeh,TRUE,Zhiwei Tong,Portfolio Loss Driven by Idiosyncratic Risks," We consider a general portfolio of assets with low default risk and evaluate the tail probability of portfolio loss due to defaults. The latent variables driving defaults are represented by the model of Bassamboo, Juneja, and Zeevi (2008), which combines systematic risk and common shock factors with idiosyncratic risk factors. While systematic risk and common shock have been found to contribute significantly to portfolio loss, the role of idiosyncratic risks is often overlooked, despite their high relevance for under-diversified or unbalanced portfolios. This study considers heavy-tailed idiosyncratic risk factors and explores two distinct scenarios: independent idiosyncratic risk factors and asymptotically dependent idiosyncratic risk factors. The former is the standard assumption in the literature. The latter is motivated by previous studies that have found the inadequacy of relying solely on common factors to capture high default correlation. This consideration is also a reflection of the possibility that idiosyncratic reasons can trigger contagion among companies with interconnected liabilities. Our main results are asymptotic equivalences for the tail of portfolio loss as the individual default probability approaches zero. These results highlight the critical role of dominating idiosyncratic risks as well as their dependence in risk assessment. 

This talk is based on joint work with Shaoying Chen and Yang Yang.","We consider a general portfolio of assets with low default risk and evaluate the tail probability of portfolio loss due to defaults. The latent variables driving defaults are represented by the model of Bassamboo, Juneja, and Zeevi (2008), which combines systematic risk and common shock factors with idiosyncratic risk factors. While systematic risk and common shock have been found to contribute significantly to portfolio loss, the role of idiosyncratic risks is often overlooked, despite their high relevance for under-diversified or unbalanced portfolios. This study considers heavy-tailed idiosyncratic risk factors and explores two distinct scenarios: independent idiosyncratic risk factors and asymptotically dependent idiosyncratic risk factors. The former is the standard assumption in the literature. The latter is motivated by previous studies that have found the inadequacy of relying solely on common factors to capture high default correlation. This consideration is also a reflection of the possibility that idiosyncratic reasons can trigger contagion among companies with interconnected liabilities. Our main results are asymptotic equivalences for the tail of portfolio loss as the individual default probability approaches zero. These results highlight the critical role of dominating idiosyncratic risks as well as their dependence in risk assessment. 

This talk is based on joint work with Shaoying Chen and Yang Yang.", ,Zhiwei Tong, ,The University of Iowa,,,Quantitative finance I
7/31/2023,A,10:50 AM,11:50 AM,12:20 PM,Aliber 103,4,Parallel Session,Quantitative Finance I,Amin Hassan Zadeh,Amin Hassan Zadeh,TRUE,Amin Hassan Zadeh,Pricing Structured Products With GMBD Rider and Mixed Fractional Brownian Motion," In recent years, insurance products related to the stock market have taken a significant share of the insurance market in the world. GMDB can be mentioned among the best-selling and famous products in this regard.
In this research, we present a new product based on the structured products, which transfers a portion of the investment risk to the policyholder, although the insurer still bears the risk of death. Based on this rider, the price of this product is comparable with almost similar products (such as GMDB). The mechanism is such that in case of death anytime in the contract duration (and before the due date), the amount of benefits is paid to the survivors at the moment of death according to the pay off function, and if alive, it is settled with him at the due date according to the contract terms.
The pricing of equity-linked insurance products is traditionally based on Brownian Motion models (BM), however the assumption of independent increments in BM is not realistic and usually the trends of risky assets have long-term dependence (recurrence).  Therefore, in this research we consider the pricing based on the Mixed Fractional Brownian motion (MFBM) process along with the jump in stock returns. It is noteworthy that MFBM avoids arbitrage by choosing the Hurst parameter correctly. 
Next, we analyze the proposed product numerically to see the impact of mortality risk according to different mortality models. Finally, we obtain the proposed product price using the predicted values from the fitted mortality model and compare it with the GMDB price.","In recent years, insurance products related to the stock market have taken a significant share of the insurance market in the world. GMDB can be mentioned among the best-selling and famous products in this regard.
In this research, we present a new product based on the structured products, which transfers a portion of the investment risk to the policyholder, although the insurer still bears the risk of death. Based on this rider, the price of this product is comparable with almost similar products (such as GMDB). The mechanism is such that in case of death anytime in the contract duration (and before the due date), the amount of benefits is paid to the survivors at the moment of death according to the pay off function, and if alive, it is settled with him at the due date according to the contract terms.
The pricing of equity-linked insurance products is traditionally based on Brownian Motion models (BM), however the assumption of independent increments in BM is not realistic and usually the trends of risky assets have long-term dependence (recurrence). Therefore, in this research we consider the pricing based on the Mixed Fractional Brownian motion (MFBM) process along with the jump in stock returns. It is noteworthy that MFBM avoids arbitrage by choosing the Hurst parameter correctly. 
Next, we analyze the proposed product numerically to see the impact of mortality risk according to different mortality models. Finally, we obtain the proposed product price using the predicted values from the fitted mortality model and compare it with the GMDB price.", ,Amin Hassan Zadeh, ,Smeal College of Business,,,Quantitative finance I
7/31/2023,A,10:50 AM,10:50 AM,11:20 AM,Aliber 107,5,Parallel Session,Retirement and Pension Mathematics I,Jean-François Bégin,Jean-François Bégin,TRUE,Dekun Zhai,The Effect of PBGC Premium Structure on Corporate Pension Funding Strategies," The Pension Benefit Guaranty Corporation (PBGC) premium burden has been an increasingly important consideration for funding management of defined benefit (DB) pension plan. Between 2008 and 2020, single-employer plan sponsors have paid over $50 billion in PBGC premiums, among which $30 billion was in the last five years. In this study, we explore the relationship between the PBGC premium structure and corporate pension funding strategies. Our empirical analysis supports the idea that sponsors of defined benefit pension plans are more likely to contribute when their funding ratio is slightly below 100%. Under the variable-rate premium structure, 20.34% of plan-year observations choose to make additional voluntary contributions in order to achieve fully or overfunded status. Also, the most severely underfunded plans are less likely to make voluntary contributions with variable-rate premium cap. Our findings suggest that a risk-based premium structure provides risk management incentives for relatively less underfunded plans to improve their financial health. However for most severely underfunded plans, these incentives are decreased and risk shifting incentives outweigh the financial impact of increased PBGC premiums, given the cap mechanism.","The Pension Benefit Guaranty Corporation (PBGC) premium burden has been an increasingly important consideration for funding management of defined benefit (DB) pension plan. Between 2008 and 2020, single-employer plan sponsors have paid over $50 billion in PBGC premiums, among which $30 billion was in the last five years. In this study, we explore the relationship between the PBGC premium structure and corporate pension funding strategies. Our empirical analysis supports the idea that sponsors of defined benefit pension plans are more likely to contribute when their funding ratio is slightly below 100%. Under the variable-rate premium structure, 20.34% of plan-year observations choose to make additional voluntary contributions in order to achieve fully or overfunded status. Also, the most severely underfunded plans are less likely to make voluntary contributions with variable-rate premium cap. Our findings suggest that a risk-based premium structure provides risk management incentives for relatively less underfunded plans to improve their financial health. However for most severely underfunded plans, these incentives are decreased and risk shifting incentives outweigh the financial impact of increased PBGC premiums, given the cap mechanism.", ,Dekun Zhai, ,Temple University,,,Retirement and pension mathematics I
7/31/2023,A,10:50 AM,11:20 AM,11:50 AM,Aliber 107,5,Parallel Session,Retirement and Pension Mathematics I,Jean-François Bégin,Jean-François Bégin,TRUE,Siegfried Anyomi,Contagion Risk in Multiemployer Pension Plans," This paper quantitatively studies the contagion risk of multiemployer pension plans (MEPPs) and the impact of employer mass withdrawals on the MEPP. We construct a recursion technique via a graph-theoretical network architecture featuring the existence of two types of employer withdrawal liability vectors. The first withdrawal liability vector captures the spread of insolvency across the MEPP network, while the second vector captures the spread of solvency. Our recursion technique estimates the employer withdrawal liability by connecting an employer’s financial position to the value of the MEPP. In practice, the proposed algorithm has a faster rate of convergence and is generally stable.","This paper quantitatively studies the contagion risk of multiemployer pension plans (MEPPs) and the impact of employer mass withdrawals on the MEPP. We construct a recursion technique via a graph-theoretical network architecture featuring the existence of two types of employer withdrawal liability vectors. The first withdrawal liability vector captures the spread of insolvency across the MEPP network, while the second vector captures the spread of solvency. Our recursion technique estimates the employer withdrawal liability by connecting an employer’s financial position to the value of the MEPP. In practice, the proposed algorithm has a faster rate of convergence and is generally stable.", ,Siegfried Anyomi, ,University of Iowa,,,Retirement and pension mathematics I
7/31/2023,A,10:50 AM,11:50 AM,12:20 PM,Aliber 107,5,Parallel Session,Retirement and Pension Mathematics I,Jean-François Bégin,Jean-François Bégin,TRUE,Jean-François Bégin,On the Impacts of Pension Plan Consolidation: Can Everyone Win?," This study investigates the benefits and drawbacks of pension plan consolidation by quantifying the impact of mergers on different stakeholders in a unique Canadian implementation of defined benefit plans. Using a comprehensive framework that combines a realistic economic scenario generator, a stochastic mortality model that captures differences among subpopulations, a cost model with economies of scale, and an asset allocation methodology based on utility maximization, three groups of measures are constructed: plan-related risk measures assessing profits from an economic capital perspective, consumption-based metrics to understand the impact on members, and contribution risk measures capturing the risk from the employer's viewpoint. We apply the framework to a hypothetical and empirically relevant merger and find that the merger is favourable under most circumstances, even with heterogenous mortality expectations, different plan maturity levels, and modest differences in initial funded ratios. 

This is joint work with Barbara Sanders and Wenyuan Zhou.","This study investigates the benefits and drawbacks of pension plan consolidation by quantifying the impact of mergers on different stakeholders in a unique Canadian implementation of defined benefit plans. Using a comprehensive framework that combines a realistic economic scenario generator, a stochastic mortality model that captures differences among subpopulations, a cost model with economies of scale, and an asset allocation methodology based on utility maximization, three groups of measures are constructed: plan-related risk measures assessing profits from an economic capital perspective, consumption-based metrics to understand the impact on members, and contribution risk measures capturing the risk from the employer's viewpoint. We apply the framework to a hypothetical and empirically relevant merger and find that the merger is favourable under most circumstances, even with heterogenous mortality expectations, different plan maturity levels, and modest differences in initial funded ratios. 

This is joint work with Barbara Sanders and Wenyuan Zhou.", ,Jean-François Bégin, ,Simon Fraser University,,,Retirement and pension mathematics I
7/31/2023,B,12:20 PM,12:20 PM,1:30 PM,Hubbell Dining Hall,NA,Lunch,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
7/31/2023,B,1:30 PM,1:30 PM,2:30 PM,Aliber 101,1,Parallel Session,(Invited Session) Professionalism in the Era of Artificial Intelligence,NA,NA,NA,Dale Hall,Professionalism in the Era of Artificial Intelligence,NA,NA,,Dale Hall ,,NA,,,Professionalism in the Era of Artificial Intelligence
7/31/2023,B,1:30 PM,1:30 PM,2:00 PM,Aliber 102,2,Parallel Session,Loss Reserving and Ratemaking,Bin Zou,Bin Zou,TRUE,Wenyi Lu,Quantification of Variability of Chain Ladder Reserve Estimates: Mack’s Method With Recursive Structure in Excel.,"This presentation can serve as a hands-on guideline for practicing P&C actuaries to build their own in-house models to quantify the range estimates of outstanding loss reserve of property & casualty insurance business in everyday work under the requirement of European Solvency II or for business plan purpose in North America. This presentation will first show explicitly how to use basic Excel functions to carry out quantifying variability of property/casualty insurance loss reserve estimates according to Thomas Mack’s CAS prize-winning paper (1993) “Measuring the Variability of Chain Ladder Reserve Estimates” formula by formula. This presentation will next show how to use basic Excel functions to fulfill the same results as Mack’s formulas in his 1993 paper via an equivalent recursive route. Mack derived the recursive formulas in his paper “The Standard Error of Chain-Ladder Reserve Estimates: Recursive Calculation and Inclusion of A Tail Factor” (1999). This presentation shows explicitly in Excel the structure of recursion in the form of matrix. All the intermediate steps are in a matrix for recursive calculations, while 1993 paper’s methods use vectors as intermediate steps. With a matrix of intermediate years’ results, this presentation shows how to include a tail factor by expanding the matrix one more row and one more column in order to extend projection of ultimate loss one more development lag. This is a true improvement for 1993’s formulas regarding flexibility and versatility. It’s worth noting that Mack’s methods in his 1993 paper can’t handle a tail factor for ultimate development because the necessary information about “The Exposure Prior to The Current Calendar Year by Development Age” is not available for the last development lag even for the oldest accident year in the data format. On the other hand, the recursive calculation method can deal with an extra tail factor as it uses the information of “The Exposure from The Current Calendar Year onward by Development Age”, which is always available. As a result, recursive formulas are more general than Mack’s formulas in 1993 paper. Overall, this presentation confirms that Mack’s model arguably is a best start point for estimating variability of loss reserves.","This presentation can serve as a hands-on guideline for practicing P&C actuaries to build their own in-house models to quantify the range estimates of outstanding loss reserve of property & casualty insurance business in everyday work under the requirement of European Solvency II or for business plan purpose in North America. This presentation will first show explicitly how to use basic Excel functions to carry out quantifying variability of property/casualty insurance loss reserve estimates according to Thomas Mack’s CAS prize-winning paper (1993) “Measuring the Variability of Chain Ladder Reserve Estimates” formula by formula. This presentation will next show how to use basic Excel functions to fulfill the same results as Mack’s formulas in his 1993 paper via an equivalent recursive route. Mack derived the recursive formulas in his paper “The Standard Error of Chain-Ladder Reserve Estimates: Recursive Calculation and Inclusion of A Tail Factor” (1999). This presentation shows explicitly in Excel the structure of recursion in the form of matrix. All the intermediate steps are in a matrix for recursive calculations, while 1993 paper’s methods use vectors as intermediate steps. With a matrix of intermediate years’ results, this presentation shows how to include a tail factor by expanding the matrix one more row and one more column in order to extend projection of ultimate loss one more development lag. This is a true improvement for 1993’s formulas regarding flexibility and versatility. It’s worth noting that Mack’s methods in his 1993 paper can’t handle a tail factor for ultimate development because the necessary information about “The Exposure Prior to The Current Calendar Year by Development Age” is not available for the last development lag even for the oldest accident year in the data format. On the other hand, the recursive calculation method can deal with an extra tail factor as it uses the information of “The Exposure from The Current Calendar Year onward by Development Age”, which is always available. As a result, recursive formulas are more general than Mack’s formulas in 1993 paper. Overall, this presentation confirms that Mack’s model arguably is a best start point for estimating variability of loss reserves.",,Wenyi Lu,,The University of Texas at Dallas,,,Loss reserving and ratemaking
7/31/2023,B,1:30 PM,2:00 PM,2:30 PM,Aliber 102,2,Parallel Session,Loss Reserving and Ratemaking,Bin Zou,Bin Zou,TRUE,Lydia Gabric,Discrimination-Free Pricing With Bayesian Variational Inference,"In recent years, many jurisdictions have implemented anti-discrimination regulations and established protected classes that cannot be used within the insurance pricing framework. Along with the increased use of complex pricing methods, it is pertinent for insurers to adhere to the evolving rules and regulations without the loss of pricing accuracy. Recent studies have recognized the possibility of direct and indirect discrimination through common pricing methods, and have established a discrimination-free pricing framework under frequentist inference. However, to ensure the resulting discrimination-free price is unbiased and attainable, the existing framework requires additional computations and techniques. Furthermore, the frequentist framework requires knowledge of the policyholder’s discriminatory information which may not be accessible to insurers due to data collection regulations. We have adapted the existing pricing framework to a Bayesian perspective through the use of Bayesian mixture models that treats the discriminatory covariates as latent variables. In conjunction with variational inference, the Bayesian methodology will naturally provide a family of discrimination-free distributions for the discriminatory variables. As such, we propose a cohesive Bayesian pricing framework that produces discrimination-free and asymptotically unbiased prices without the use of policyholder’s discriminatory information.","In recent years, many jurisdictions have implemented anti-discrimination regulations and established protected classes that cannot be used within the insurance pricing framework. Along with the increased use of complex pricing methods, it is pertinent for insurers to adhere to the evolving rules and regulations without the loss of pricing accuracy. Recent studies have recognized the possibility of direct and indirect discrimination through common pricing methods, and have established a discrimination-free pricing framework under frequentist inference. However, to ensure the resulting discrimination-free price is unbiased and attainable, the existing framework requires additional computations and techniques. Furthermore, the frequentist framework requires knowledge of the policyholder’s discriminatory information which may not be accessible to insurers due to data collection regulations. We have adapted the existing pricing framework to a Bayesian perspective through the use of Bayesian mixture models that treats the discriminatory covariates as latent variables. In conjunction with variational inference, the Bayesian methodology will naturally provide a family of discrimination-free distributions for the discriminatory variables. As such, we propose a cohesive Bayesian pricing framework that produces discrimination-free and asymptotically unbiased prices without the use of policyholder’s discriminatory information.", ,Lydia Gabric, ,Arizona State Univeristy,,,Loss reserving and ratemaking
7/31/2023,B,1:30 PM,2:30 PM,3:00 PM,Aliber 102,2,Parallel Session,Loss Reserving and Ratemaking,Bin Zou,Bin Zou,TRUE,Bin Zou,Linear Classifier Models for Binary Classification,"We propose a class of linear classifier models and consider a flexible loss function to study binary classification problems. The loss function consists of two penalty terms, one penalizing false positive (FP) and the other penalizing false negative (FN), and can accommodate various classification targets by choosing a weighting function to adjust the impact of FP and FN on classification. We show, through both a simulated study and an empirical analysis, that the proposed models under certain parametric weight functions outperform the logistic regression model and can be trained to meet flexible targeted rates on FP or FN.","We propose a class of linear classifier models and consider a flexible loss function to study binary classification problems. The loss function consists of two penalty terms, one penalizing false positive (FP) and the other penalizing false negative (FN), and can accommodate various classification targets by choosing a weighting function to adjust the impact of FP and FN on classification. We show, through both a simulated study and an empirical analysis, that the proposed models under certain parametric weight functions outperform the logistic regression model and can be trained to meet flexible targeted rates on FP or FN.", ,Bin Zou, ,University of Connecticut,,,Loss reserving and ratemaking
7/31/2023,B,1:30 PM,1:30 PM,2:00 PM,Aliber 103,3,Parallel Session,Climate Risk and Sustainability I,Zhiwei Tong,Zhiwei Tong,TRUE,Qihe Tang,Understanding Compound Weather and Climate Events Through the Lens of Systemic Financial Crises,"Climate change has emerged as one of the greatest global challenges of our time. One of the most concerning risks of climate change is the potential for compound events (CEs) to increase, where multiple climate drivers and/or hazards interact with each other in a complex manner, leading to amplified impacts on the physical environment, human systems, and ecosystems. To achieve a quantitative understanding of CEs, we develop a modeling framework that comprises climate drivers, hazard events, and damages and addresses the stochastic nature of CEs. Motivated by recent studies on complex financial networks and the long-standing concept of systemic risk, we liken CEs to systemic crises in complex financial networks. Despite apparent differences between the two concepts, CEs and financial crises share strong similarities. CEs are influenced by climate change, which, in turn, is caused by carbon emissions. Similarly, financial crises are driven by factors like excessive risk-taking, macroeconomic imbalances, and regulatory failures, all of which can be traced back to human greed. Therefore, some of their similarities may be attributed to their common anthropogenic nature. The fully stochastic and dynamic nature of our modeling framework makes it feasible to accommodate some important features of CEs, such as spatio-temporal dependencies, causal chains, feedback loops, tipping points, nonlinearity, and deep uncertainty.","Climate change has emerged as one of the greatest global challenges of our time. One of the most concerning risks of climate change is the potential for compound events (CEs) to increase, where multiple climate drivers and/or hazards interact with each other in a complex manner, leading to amplified impacts on the physical environment, human systems, and ecosystems. To achieve a quantitative understanding of CEs, we develop a modeling framework that comprises climate drivers, hazard events, and damages and addresses the stochastic nature of CEs. Motivated by recent studies on complex financial networks and the long-standing concept of systemic risk, we liken CEs to systemic crises in complex financial networks. Despite apparent differences between the two concepts, CEs and financial crises share strong similarities. CEs are influenced by climate change, which, in turn, is caused by carbon emissions. Similarly, financial crises are driven by factors like excessive risk-taking, macroeconomic imbalances, and regulatory failures, all of which can be traced back to human greed. Therefore, some of their similarities may be attributed to their common anthropogenic nature. The fully stochastic and dynamic nature of our modeling framework makes it feasible to accommodate some important features of CEs, such as spatio-temporal dependencies, causal chains, feedback loops, tipping points, nonlinearity, and deep uncertainty.",,Qihe Tang, ,UNSW Sydney,,,Climate risk and sustainability I
7/31/2023,B,1:30 PM,2:00 PM,2:30 PM,Aliber 103,3,Parallel Session,Climate Risk and Sustainability I,Zhiwei Tong,Zhiwei Tong,TRUE,Yuhao (Howard) Liu,Utility Indifference Pricing of Green Bonds,"The green bond market was born in 2007, started to accelerate from 2014, and has reached US$2.334 trillion globally as of June 2023. Green bonds, as an innovative financial instrument, have played an important role in capital mobilization towards climate change mitigation and adaptation. This study aims at the pricing of a single-period defaultable green bond. Main challenges of the pricing task include the following: First, the prices of financial assets are significantly influenced by the financial risks induced by climate change; Second, and more importantly, empirical research on Environment, Social, and Governance (ESG) investing reveals that investors may derive non-pecuniary benefits from green asset investments. We develop a utility indifference pricing framework that encapsulates both the climate-related financial risks and the non-pecuniary benefits from green investments. This framework provides a probabilistic representation of the utility indifference price of the bond. Moreover, we conduct comprehensive numerical analyses to examine the effects of the climate risk and non-pecuniary benefits on the price.","The green bond market was born in 2007, started to accelerate from 2014, and has reached US$2.334 trillion globally as of June 2023. Green bonds, as an innovative financial instrument, have played an important role in capital mobilization towards climate change mitigation and adaptation. This study aims at the pricing of a single-period defaultable green bond. Main challenges of the pricing task include the following: First, the prices of financial assets are significantly influenced by the financial risks induced by climate change; Second, and more importantly, empirical research on Environment, Social, and Governance (ESG) investing reveals that investors may derive non-pecuniary benefits from green asset investments. We develop a utility indifference pricing framework that encapsulates both the climate-related financial risks and the non-pecuniary benefits from green investments. This framework provides a probabilistic representation of the utility indifference price of the bond. Moreover, we conduct comprehensive numerical analyses to examine the effects of the climate risk and non-pecuniary benefits on the price.",,Yuhao (Howard) Liu, ,UNSW Australia,,,Climate risk and sustainability II
7/31/2023,B,1:30 PM,2:30 PM,3:00 PM,Aliber 103,3,Parallel Session,Climate Risk and Sustainability I,Zhiwei Tong,Zhiwei Tong,TRUE,Sebastien Jessup,Impact of Combination Methods on Extreme Precipitation Projections,"Climate change is expected to impact the frequency and intensity of extreme weather events. Actuaries seeking to determine the economic risk of these events must often rely on expert models from multiple sources. These actuaries then need combination techniques to evaluate the quality of expert-provided information. Different combination methods use varying assumptions, which can lead to very different results. In this talk, we apply non-parametric and Bayesian combination techniques to an ensemble of 24 experts through weighted pooling. We use the differences in outputs to highlight how different combination methods can yield greater insight into projection uncertainty.","Climate change is expected to impact the frequency and intensity of extreme weather events. Actuaries seeking to determine the economic risk of these events must often rely on expert models from multiple sources. These actuaries then need combination techniques to evaluate the quality of expert-provided information. Different combination methods use varying assumptions, which can lead to very different results. In this talk, we apply non-parametric and Bayesian combination techniques to an ensemble of 24 experts through weighted pooling. We use the differences in outputs to highlight how different combination methods can yield greater insight into projection uncertainty.", ,Sebastien Jessup, ,Concordia University,,,Risk modeling and measurement I
7/31/2023,B,1:30 PM,1:30 PM,2:00 PM,Aliber 107,4,Parallel Session,Optimal Control and Optimization I,Bin Li,Bin Li,TRUE,Xiaoyu Song,Dynamic Utility Maximization With Stochastic Health Risk and Lifetime Income,"We study optimal savings and consumption choices under uncertain lifetime and stochastic health risk. An extensive literature considers individuals’ decisions over their life-cycle in the context of an uncertain lifetime following Yaari’s seminal work. The previous research has primarily studied settings with deterministic health conditions. We extend the life-cycle framework to allow for stochastic health and mortality in a continuous-time setting with survival-contingent income. Exact solutions in closed-form for individuals' optimal consumption paths are solved and a solution algorithm with linear computation speed is proposed. Following the linearity, we show that the risk environment can be generalized beyond health risk. ","We study optimal savings and consumption choices under uncertain lifetime and stochastic health risk. An extensive literature considers individuals’ decisions over their life-cycle in the context of an uncertain lifetime following Yaari’s seminal work. The previous research has primarily studied settings with deterministic health conditions. We extend the life-cycle framework to allow for stochastic health and mortality in a continuous-time setting with survival-contingent income. Exact solutions in closed-form for individuals' optimal consumption paths are solved and a solution algorithm with linear computation speed is proposed. Following the linearity, we show that the risk environment can be generalized beyond health risk.", ,Xiaoyu Song, ,University of Wisconsin – Madison,,,Optimal control and optimization
7/31/2023,B,1:30 PM,2:00 PM,2:30 PM,Aliber 107,4,Parallel Session,Optimal Control and Optimization I,Bin Li,Bin Li,TRUE,Kenneth Ng,Variable Annuities With a Performance Fee Structure: A Way Out of the Policyholder-Insurer Conflict,"Variable annuities allow policyholders to participate in the equity market. Due to the protection provided by the insurer, policyholders tend to take a more risky investment position, creating a potential conflict between the insurer and the policyholder. We investigate empirically the investment restrictions imposed by insurers in guaranteed lifetime withdrawal benefits (GLWBs), and find that insurers restrict policyholders to safer investments. This effect is more prominent when the interests of insurers and policyholders diverge further, e.g., when the insurance protection is high, when the insurers are less capable to adjust fee rates, when the market is more volatile, and after crisis. To mitigate this conflict, we formulate a mathematical model to study the role of different fee structures for GLWBs with a ratchet feature and provide analytical solutions of the optimal control problem for both the constant fee and performance-based fee incentives. Verification theorems are also provided, which involve non-canonical estimations due to the ratchet feature. The model illustrates that the performance fee structure is prone to alleviate the aforementioned conflict. Specifically, policyholders tend to take a less risky investment portfolio compared to its constant fee counterpart, which reduces the likelihood of account depletion, and delivers more reliable profits to the insurer. These substantiate the importance of designing an adequate fee incentive for variable annuities.","Variable annuities allow policyholders to participate in the equity market. Due to the protection provided by the insurer, policyholders tend to take a more risky investment position, creating a potential conflict between the insurer and the policyholder. We investigate empirically the investment restrictions imposed by insurers in guaranteed lifetime withdrawal benefits (GLWBs), and find that insurers restrict policyholders to safer investments. This effect is more prominent when the interests of insurers and policyholders diverge further, e.g., when the insurance protection is high, when the insurers are less capable to adjust fee rates, when the market is more volatile, and after crisis. To mitigate this conflict, we formulate a mathematical model to study the role of different fee structures for GLWBs with a ratchet feature and provide analytical solutions of the optimal control problem for both the constant fee and performance-based fee incentives. Verification theorems are also provided, which involve non-canonical estimations due to the ratchet feature. The model illustrates that the performance fee structure is prone to alleviate the aforementioned conflict. Specifically, policyholders tend to take a less risky investment portfolio compared to its constant fee counterpart, which reduces the likelihood of account depletion, and delivers more reliable profits to the insurer. These substantiate the importance of designing an adequate fee incentive for variable annuities.", ,Kenneth Ng, ,University of Illinois at Urbana-Champaign,,,Optimal control and optimization
7/31/2023,B,1:30 PM,2:30 PM,3:00 PM,Aliber 107,4,Parallel Session,Optimal Control and Optimization I,Bin Li,Bin Li,TRUE,Bin Li,Risk-Sharing Pricing of Variable Annuities Within a Principal-Agent Framework," We propose a new risk-sharing pricing approach of variable annuities within a principal-agent framework where an insurer (principal) is the contract provider and a policyholder (agent) is the follower having the surrender option. While the risk neutral pricing approach adopted in the existing literature leads to signi?cantly higher fees and more frequent surrendering than market observations, this new risk-sharing pricing approach reconciles the misalignment between theoretical results and market observations.","We propose a new risk-sharing pricing approach of variable annuities within a principal-agent framework where an insurer (principal) is the contract provider and a policyholder (agent) is the follower having the surrender option. While the risk neutral pricing approach adopted in the existing literature leads to signi?cantly higher fees and more frequent surrendering than market observations, this new risk-sharing pricing approach reconciles the misalignment between theoretical results and market observations.", ,Bin Li, ,University of Waterloo,,,Optimal control and optimization
7/31/2023,B,1:30 PM,1:30 PM,2:00 PM,Aliber 108,5,Parallel Session,Education and Professional Development I,Russell Hendel,Russell Hendel,TRUE,Jeffrey Zheng,Educational Technology Tools (Including ChatGPT) That Enhance the Student Experience (and Make Your Life Easier!),"As technology continues to evolve and impact our daily lives, it is increasingly important to incorporate educational technology tools in the classroom to enhance the student experience and streamline educators' workflow.  Given the sheer volume of options available, what can both save you time as an educator, while boosting student learning and engagement?  This session will include a sampling of ideas of tools related to course management, content delivery, feedback, visualization, collaboration, polling, discussion.  Some of these resources can be used in the classroom, while others improve interactivity and efficiency outside of the classroom.  Given the rise of AI, the session will engage attendees in an interactive discussion of how ChatGPT can be used as a positive force in education, such as providing personalized feedback, instead of solely serving as a source of plagiarism anxiety.
Pre-Session Homework: Which 1 sentence in this abstract was written by ChatGPT?","As technology continues to evolve and impact our daily lives, it is increasingly important to incorporate educational technology tools in the classroom to enhance the student experience and streamline educators' workflow. Given the sheer volume of options available, what can both save you time as an educator, while boosting student learning and engagement? This session will include a sampling of ideas of tools related to course management, content delivery, feedback, visualization, collaboration, polling, discussion. Some of these resources can be used in the classroom, while others improve interactivity and efficiency outside of the classroom. Given the rise of AI, the session will engage attendees in an interactive discussion of how ChatGPT can be used as a positive force in education, such as providing personalized feedback, instead of solely serving as a source of plagiarism anxiety.
Pre-Session Homework: Which 1 sentence in this abstract was written by ChatGPT?",,Jeffrey Zheng, ,Temple University,,,Education and Professional Development I
7/31/2023,B,1:30 PM,2:00 PM,2:30 PM,Aliber 108,5,Parallel Session,Education and Professional Development I,Russell Hendel,Russell Hendel,TRUE,John Sang Jin Kang,Artificial Intelligence (AI): An Ally or a Foe for the Actuarial Education?,"The surge of the artificial intelligence (AI) drastically changes the the educational practices in the post-secondary education institute: including the fear about the impact of the academic integrity, the dilemma of ethics in education, but the opportunities of innovative teaching practice and assessments.
Inspired by Dr. Sarah Eaton's talk on the pre-conference workshop ""Academic Integrity in an Age of Educational Transformation (or: Why Robots Won’t Inherit the Earth)"" during the Conference on Post-secondary Learning and Teaching by the Taylor Institute of Teaching and Learning in the University of Calgary, I would like to share how the AI gives potential impact on teaching and learning practice in the actuarial education. This talk allows the opportunities for the participants to share their sentiment and to engage in the dialogue for the readiness to this emerging tool. In addition, the strive of professionalism and daily ethical practice provides the safeguard for teaching and learning practice on using the AI tools.
This talk is the beginning step for articulating the Scholars of Teaching and Learning (SoTL) for the issue of the ethical practice of using the AI in the actuarial science and data science.","The surge of the artificial intelligence (AI) drastically changes the the educational practices in the post-secondary education institute: including the fear about the impact of the academic integrity, the dilemma of ethics in education, but the opportunities of innovative teaching practice and assessments.
Inspired by Dr. Sarah Eaton's talk on the pre-conference workshop ""Academic Integrity in an Age of Educational Transformation (or: Why Robots Won’t Inherit the Earth)"" during the Conference on Post-secondary Learning and Teaching by the Taylor Institute of Teaching and Learning in the University of Calgary, I would like to share how the AI gives potential impact on teaching and learning practice in the actuarial education. This talk allows the opportunities for the participants to share their sentiment and to engage in the dialogue for the readiness to this emerging tool. In addition, the strive of professionalism and daily ethical practice provides the safeguard for teaching and learning practice on using the AI tools.
This talk is the beginning step for articulating the Scholars of Teaching and Learning (SoTL) for the issue of the ethical practice of using the AI in the actuarial science and data science.",,John Sang Jin Kang, ,University of Calgary,,,Education and Professional Development I
7/31/2023,B,1:30 PM,2:30 PM,3:00 PM,Aliber 108,5,Parallel Session,Education and Professional Development I,Russell Hendel,Russell Hendel,TRUE,Russell Hendel,The Syllabus as an Instructional Aid for Complex Problem Solving,"The course syllabus serves varied functions [4] including being a: i) contract between the university, the instructor, and student, ii) means of achieving uniformity and accountability across multiple sections of the same course taught by different instructors, iii) means of evaluating the readiness of transfer students from other universities to take courses having certain prerequisites, iv) means of evaluation of an instructor's and institution's compliance with required coverage of certain topics enabling various recognitions, statuses, promotions, and certifications, v) permanent record that future faculty can use when teaching the course, and vi) a schedule of topics and exams useful for instructors and students to plan.

This article studies the syllabus as a dynamic interactive teaching tool that informs and is informed by both solutions of complex course problems and student difficulties with these problems. 

Two examples are presented: i) By considering the prerequisite sequence of modules needed to solve complex problems, the course problems can shape and help create the syllabus design; ii) by skillfully using lessons learned in sub-tasking the component parts of a problem solution, the syllabus dynamically grows enabling significant increase in student learning, retention, and satisfaction. The techniques reviewed enable a deeper understanding of goal-setting, the process by which optimal sub-tasking of component parts of a solution is achieved.","The course syllabus serves varied functions [4] including being a: i) contract between the university, the instructor, and student, ii) means of achieving uniformity and accountability across multiple sections of the same course taught by different instructors, iii) means of evaluating the readiness of transfer students from other universities to take courses having certain prerequisites, iv) means of evaluation of an instructor's and institution's compliance with required coverage of certain topics enabling various recognitions, statuses, promotions, and certifications, v) permanent record that future faculty can use when teaching the course, and vi) a schedule of topics and exams useful for instructors and students to plan.

This article studies the syllabus as a dynamic interactive teaching tool that informs and is informed by both solutions of complex course problems and student difficulties with these problems. 

Two examples are presented: i) By considering the prerequisite sequence of modules needed to solve complex problems, the course problems can shape and help create the syllabus design; ii) by skillfully using lessons learned in sub-tasking the component parts of a problem solution, the syllabus dynamically grows enabling significant increase in student learning, retention, and satisfaction. The techniques reviewed enable a deeper understanding of goal-setting, the process by which optimal sub-tasking of component parts of a solution is achieved.",,Russell Hendel, ,Towson University,,,Education and Professional Development I
7/31/2023,C,3:00 PM,3:00 PM,3:20 PM,Aliber Hall,NA,Coffee Break,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
7/31/2023,C,3:20 PM,3:20 PM,4:20 PM,Aliber 101,1,Parallel Session,(Invited Session) Casualty Actuarial Society Invited Session,Linfeng Zhang,Linfeng Zhang,TRUE,Peng Shi and Brian Fanin,Unlocking the Value of CAS Research,"In today’s actuarial profession, practical research plays a pivotal role in driving progress and innovation. The session aims to educate participants about the fundamental role of CAS research initiatives across its many publications, with special attention paid to contributions to Variance, CAS’s peer-reviewed journal, and research in the ratemaking specialty. The session will also highlight opportunities for researchers and institutions to contribute to the CAS’s research catalogue.","In today’s actuarial profession, practical research plays a pivotal role in driving progress and innovation. The session aims to educate participants about the fundamental role of CAS research initiatives across its many publications, with special attention paid to contributions to Variance, CAS’s peer-reviewed journal, and research in the ratemaking specialty. The session will also highlight opportunities for researchers and institutions to contribute to the CAS’s research catalogue.", ,Peng Shi,Brian Fanin,, , ,Casualty Actuarial Society Invited Session
7/31/2023,C,3:20 PM,3:20 PM,3:50 PM,Aliber 102,2,Parallel Session,"Blockchain, Telematics, and InsurTech I",Xiaochen Jing,Xiaochen Jing,TRUE,Panyi Dong,Privacy-Preserving Collaborative Information Sharing Through Federated Learning – A Case of the Insurance Industry," The report proposes the first comprehensive analysis to unlock restricting financial data by harnessing the value of Federated Learning (FL) in the insurance industry through sharing industry-level insights and expertise while enhancing privacy. The application of FL addresses two of the most pressing concerns: limited data volume and data variety, which are caused by privacy concerns, the rarity of claim events, and the lack of informative rating factors, etc. in the insurance industry. Specifically, through collaborations among insurance companies and cooperation between insurance companies and InsurTech, each private ""data owner"" trains a model locally using only their data, which is then shared with the central ""aggregation server"" to create a consensus model with accumulated knowledge from all ""data owners."" Additionally, the open-source framework, OpenFL, used in our experiments is designed so that it can be run using confidential computing and with additional algorithmic protection against information leakage via the shared model updates. This proposed FL method can create a more comprehensive picture of risk assessment of policyholders' status with unprecedented effectiveness and efficiency in forecasting claim losses for small and mid-sized insured businesses. This approach enables machine learning innovations even in cases where data centers or computing servers cannot collect centralized datasets, making it a privacy-enhancing collaborative learning technique that addresses the challenges posed by the sensitivity and privacy of data in traditional machine learning solutions. This paper's application of FL can also be expanded to other areas, including fraud detection, catastrophe modeling, etc., that have similar concerns incorporating data privacy and machine learning strategies. Our framework and empirical results provide a foundation for future collaborations among insurers, regulators, academic researchers, and InsurTech experts.","The report proposes the first comprehensive analysis to unlock restricting financial data by harnessing the value of Federated Learning (FL) in the insurance industry through sharing industry-level insights and expertise while enhancing privacy. The application of FL addresses two of the most pressing concerns: limited data volume and data variety, which are caused by privacy concerns, the rarity of claim events, and the lack of informative rating factors, etc. in the insurance industry. Specifically, through collaborations among insurance companies and cooperation between insurance companies and InsurTech, each private ""data owner"" trains a model locally using only their data, which is then shared with the central ""aggregation server"" to create a consensus model with accumulated knowledge from all ""data owners."" Additionally, the open-source framework, OpenFL, used in our experiments is designed so that it can be run using confidential computing and with additional algorithmic protection against information leakage via the shared model updates. This proposed FL method can create a more comprehensive picture of risk assessment of policyholders' status with unprecedented effectiveness and efficiency in forecasting claim losses for small and mid-sized insured businesses. This approach enables machine learning innovations even in cases where data centers or computing servers cannot collect centralized datasets, making it a privacy-enhancing collaborative learning technique that addresses the challenges posed by the sensitivity and privacy of data in traditional machine learning solutions. This paper's application of FL can also be expanded to other areas, including fraud detection, catastrophe modeling, etc., that have similar concerns incorporating data privacy and machine learning strategies. Our framework and empirical results provide a foundation for future collaborations among insurers, regulators, academic researchers, and InsurTech experts.", ,Panyi Dong, ,University of Illinois Urbana-Champaign,,,"Blockchain, Telematics, and InsurTech II"
7/31/2023,C,3:20 PM,3:50 PM,4:20 PM,Aliber 102,2,Parallel Session,"Blockchain, Telematics, and InsurTech I",Xiaochen Jing,Xiaochen Jing,TRUE,Xiaochen Jing,Capacity Pricing in Decentralized Insurance,"The Proof-of-Stake (PoS) consensus from blockchain technology has recently gained significant attention due to its advantages in energy efficiency, decentralization, and scalability. While PoS validators stake to validate transactions and resolve disputes, the same mechanism has been 	introduced to decentralized insurance for risk and claim assessment purposes. Although stakers have a financial incentive to act honestly because they have staked their own funds as collateral, the existing pricing mechanisms in decentralized insurance networks do not adequately account for the underlying riskiness, resulting in consistent losses for the business. In this paper, we explore a new on-chain pricing model that takes both the market demand and staking capacity into consideration. While it mitigates the underpricing problem, the new mechanism relies solely on staking pool managers’ expertise to uncover riskiness and may cause unintended managerial behaviors related to capacity management and competition.","The Proof-of-Stake (PoS) consensus from blockchain technology has recently gained significant attention due to its advantages in energy efficiency, decentralization, and scalability. While PoS validators stake to validate transactions and resolve disputes, the same mechanism has been 	introduced to decentralized insurance for risk and claim assessment purposes. Although stakers have a financial incentive to act honestly because they have staked their own funds as collateral, the existing pricing mechanisms in decentralized insurance networks do not adequately account for the underlying riskiness, resulting in consistent losses for the business. In this paper, we explore a new on-chain pricing model that takes both the market demand and staking capacity into consideration. While it mitigates the underpricing problem, the new mechanism relies solely on staking pool managers’ expertise to uncover riskiness and may cause unintended managerial behaviors related to capacity management and competition.",,Xiaochen Jing, ,University of Illinois at Urbana-Champaign,,,"Blockchain, Telematics, and InsurTech II"
7/31/2023,C,3:20 PM,3:20 PM,3:50 PM,Aliber 103,3,Parallel Session,Risk Modeling and Measurement I,Nariankadu Shymalkuma,Nariankadu Shymalkuma,TRUE,Rui Zhou,A Probabilistic Risk Analysis Framework for Biofouling Insurance Pricing," This paper presents a research framework to develop a probabilistic risk analysis for biofouling insurance pricing. The proposed biofouling insurance is mandatory for all vessels entering Australian ports. It is designed to promote preventative measures and mitigate the introduction of non-indigenous marine species (NIMS) to Australia. We establish a risk-based pricing framework that incorporates the expenses associated with monitoring biofouling risk and the response losses resulting from NIMS incursions. This study synergizes actuarial techniques and scientific findings to develop a biofouling insurance pricing model. The proposed approach involves: (1) developing a probabilistic method to analyze biofouling risk, generating incursion scenarios and evaluating potential losses, and (2) applying actuarial principles to determine premiums for individual vessels based on their riskiness and the loss distribution derived from the probabilistic risk analysis. By leveraging existing research and expert judgement to address data gaps, this study seeks to establish a comprehensive framework for biofouling insurance pricing that incentivizes ship owners to implement more effective preventive measures.","This paper presents a research framework to develop a probabilistic risk analysis for biofouling insurance pricing. The proposed biofouling insurance is mandatory for all vessels entering Australian ports. It is designed to promote preventative measures and mitigate the introduction of non-indigenous marine species (NIMS) to Australia. We establish a risk-based pricing framework that incorporates the expenses associated with monitoring biofouling risk and the response losses resulting from NIMS incursions. This study synergizes actuarial techniques and scientific findings to develop a biofouling insurance pricing model. The proposed approach involves: (1) developing a probabilistic method to analyze biofouling risk, generating incursion scenarios and evaluating potential losses, and (2) applying actuarial principles to determine premiums for individual vessels based on their riskiness and the loss distribution derived from the probabilistic risk analysis. By leveraging existing research and expert judgement to address data gaps, this study seeks to establish a comprehensive framework for biofouling insurance pricing that incentivizes ship owners to implement more effective preventive measures.", ,Rui Zhou, ,University of Melbourne,,,Risk modeling and measurement III
7/31/2023,C,3:20 PM,3:50 PM,4:20 PM,Aliber 103,3,Parallel Session,Risk Modeling and Measurement I,Nariankadu Shymalkuma,Nariankadu Shymalkuma,TRUE,Vytaras Brazauskas,Measuring Discrete Risks on Infinite Domains," To accommodate numerous practical scenarios, in this paper we extend statistical inference for smoothed quantile estimators from finite domains to infinite domains. We accomplish the task with the help of a newly designed truncation methodology for discrete loss distributions with infinite domains. A simulation study illustrates the methodology in the case of  several distributions, such as Poisson, negative binomial, and their zero inflated versions, which are commonly used in insurance industry to model claim frequencies. Additionally, we propose a very flexible bootstrap-based approach for the use in practice. Using automobile accident data and their modifications, we compute what we have termed the conditional five number summary (C5NS) for the tail risk and construct confidence intervals for each of the five quantiles making up C5NS, and then calculate the tail probabilities. The results show that the smoothed quantile approach classifies the tail riskiness of portfolios not only more accurately but also produces lower coefficients of variation in the estimation of tail probabilities than those obtained using the linear interpolation approach.","To accommodate numerous practical scenarios, in this paper we extend statistical inference for smoothed quantile estimators from finite domains to infinite domains. We accomplish the task with the help of a newly designed truncation methodology for discrete loss distributions with infinite domains. A simulation study illustrates the methodology in the case of several distributions, such as Poisson, negative binomial, and their zero inflated versions, which are commonly used in insurance industry to model claim frequencies. Additionally, we propose a very flexible bootstrap-based approach for the use in practice. Using automobile accident data and their modifications, we compute what we have termed the conditional five number summary (C5NS) for the tail risk and construct confidence intervals for each of the five quantiles making up C5NS, and then calculate the tail probabilities. The results show that the smoothed quantile approach classifies the tail riskiness of portfolios not only more accurately but also produces lower coefficients of variation in the estimation of tail probabilities than those obtained using the linear interpolation approach.", ,Vytaras Brazauskas, ,University of Wisconsin-Milwaukee,,,Risk modeling and measurement I
7/31/2023,C,3:20 PM,3:20 PM,3:50 PM,Aliber 107,4,Parallel Session,P & C Insurance,Wei Wei,Wei Wei,TRUE,Spark Tseung,Improving a Posteriori Risk Classification and Ratemaking With Random Effects in the Mixture of Experts Model," In the underwriting and pricing of non-life insurance products, it is essential for the insurer to utilize both policyholder information and claim history. We apply a flexible regression model with random effects, called the Mixed LRMoE to categorize policyholders into groups with similar risk profiles, and to determine a premium that accurately captures the unobserved risks. Our proposed framework is shown to outperform the classical benchmark models (Logistic and Lognormal GL(M)M) in terms of goodness-of-fit to data, while offering intuitive and interpretable characterization of policyholders' risk profiles to adequately reflect their claim history.","In the underwriting and pricing of non-life insurance products, it is essential for the insurer to utilize both policyholder information and claim history. We apply a flexible regression model with random effects, called the Mixed LRMoE to categorize policyholders into groups with similar risk profiles, and to determine a premium that accurately captures the unobserved risks. Our proposed framework is shown to outperform the classical benchmark models (Logistic and Lognormal GL(M)M) in terms of goodness-of-fit to data, while offering intuitive and interpretable characterization of policyholders' risk profiles to adequately reflect their claim history.", ,Spark Tseung, ,University of Toronto,,,P & C insurance
7/31/2023,C,3:20 PM,3:50 PM,4:20 PM,Aliber 107,4,Parallel Session,P & C Insurance,Wei Wei,Wei Wei,TRUE,Raïssa Coulibaly,Bonus-Malus Scale Models for Claim Severities and Total Claim Amount," In recent literature, Boucher (2022) used Bonus-Malus Scales (BMS) models to model the decrease and increase in premiums for respectively to policyholders who do not claim and who claim. His paper shows that the BMS models have an excellent quality of adjustment and prediction compared to the classical pricing models using panel data. While Boucher (2022) focuses only on modeling the annual claims number of a contract, our project aims to generalize the approach to cover the modeling of the yearly loss cost of this contract. Based on the recent paper by Delong and al. (2022), two distinct approaches to loss cost modeling are investigated. So, first, we present the BMS models using a frequency-severity approach (Poisson-gamma). Second, we consider the BMS models using the loss cost distribution (Tweedie) directly. Thus, our proposed BMS models generalize Delong and al. (2022) to include a predictive pricing component based on past claims history. The models are applied to a sample of data from the portfolio of a major insurance company in Canada. An analysis of data fit and predictability is performed. We show that the studied models are exciting alternatives to consider from a practical point of view and how predictive pricing models can address some important practical considerations.","In recent literature, Boucher (2022) used Bonus-Malus Scales (BMS) models to model the decrease and increase in premiums for respectively to policyholders who do not claim and who claim. His paper shows that the BMS models have an excellent quality of adjustment and prediction compared to the classical pricing models using panel data. While Boucher (2022) focuses only on modeling the annual claims number of a contract, our project aims to generalize the approach to cover the modeling of the yearly loss cost of this contract. Based on the recent paper by Delong and al. (2022), two distinct approaches to loss cost modeling are investigated. So, first, we present the BMS models using a frequency-severity approach (Poisson-gamma). Second, we consider the BMS models using the loss cost distribution (Tweedie) directly. Thus, our proposed BMS models generalize Delong and al. (2022) to include a predictive pricing component based on past claims history. The models are applied to a sample of data from the portfolio of a major insurance company in Canada. An analysis of data fit and predictability is performed. We show that the studied models are exciting alternatives to consider from a practical point of view and how predictive pricing models can address some important practical considerations.", ,Raïssa Coulibaly, ,UQAM,,,P & C insurance
7/31/2023,C,3:20 PM,3:20 PM,3:50 PM,Aliber 108,5,Parallel Session,Climate Risk and Sustainability II,Lisa Gao,Lisa Gao,TRUE,Jiayi Guo and Xiaoying Zhang,Estimating the Climate-Change Impact on a Fixed Income Investment Portfolio Due to Physical and Transitional Risks,"In light of the rapid and notable changes in the global climate, it is crucial to acknowledge the potential implications for financial markets and investments. Therefore, in this study, we develop a flexible mathematical model to evaluate the fundamental climate risks within fixed-income investment portfolios, offering insights applicable to finance and sustainability. Our bond-type agnostic model incorporates an explicit dependence structure among physical risk factors through the utilization of a t copula. Furthermore, based on the National Financial Global Scenarios (NFGS) and the Intergovernmental Panel on Climate Change (IPCC) pathways, we obtain coherent risk measures, such as stress tests using conditional value-at-risk (TVaR), under various scenarios. In this study, we selected the Philippines as the primary subject due to its high vulnerability to and frequent exposure to extreme natural hazards.","In light of the rapid and notable changes in the global climate, it is crucial to acknowledge the potential implications for financial markets and investments. Therefore, in this study, we develop a flexible mathematical model to evaluate the fundamental climate risks within fixed-income investment portfolios, offering insights applicable to finance and sustainability. Our bond-type agnostic model incorporates an explicit dependence structure among physical risk factors through the utilization of a t copula. Furthermore, based on the National Financial Global Scenarios (NFGS) and the Intergovernmental Panel on Climate Change (IPCC) pathways, we obtain coherent risk measures, such as stress tests using conditional value-at-risk (TVaR), under various scenarios. In this study, we selected the Philippines as the primary subject due to its high vulnerability to and frequent exposure to extreme natural hazards.",,Jiayi Guo, Xiaoying Zhang,Department of Mathematics of UIUC (Actuarial Science),,,Climate risk and sustainability II
7/31/2023,C,3:20 PM,3:50 PM,4:20 PM,Aliber 108,5,Parallel Session,Climate Risk and Sustainability II,Lisa Gao,Lisa Gao,TRUE,Esther Boyle,Joint Loss Model for Convective Storm Property Damage: Matrix Variate Time Series Bilinear Factor Analysis Approach,"While there are a multitude of models in the literature to quantify losses due to natural hazards, studies focused on joint losses from convective storms are lacking. This research investigates the spatio-temporal loss distribution resulting from convective storms across the United States. Our developed matrix variate bilinear factor analyzer model allows for simultaneous modeling of losses associated with different perils across space and time, allowing for correlation in losses. The factor based approach is able to account for unforeseen forces that drive joint losses. By drawing on county level spatio-temporal disaster data going back to 1960, we will capture evolving joint loss patterns over time and forecast future losses.","While there are a multitude of models in the literature to quantify losses due to natural hazards, studies focused on joint losses from convective storms are lacking. This research investigates the spatio-temporal loss distribution resulting from convective storms across the United States. Our developed matrix variate bilinear factor analyzer model allows for simultaneous modeling of losses associated with different perils across space and time, allowing for correlation in losses. The factor based approach is able to account for unforeseen forces that drive joint losses. By drawing on county level spatio-temporal disaster data going back to 1960, we will capture evolving joint loss patterns over time and forecast future losses.",,Esther Boyle, ,Arizona State University,,,Climate risk and sustainability II
7/31/2023,C,5:00 PM,5:00 PM,7:00 PM,Jasper Winery,NA,Jasper Winery Tour (Shuttle departs from Drake for Jasper Winery at 4:40 PM and 5:10 PM),NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
8/1/2023,D,8:30 AM,8:30 AM,9:00 AM,Aliber Hall,NA,Breakfast,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
8/1/2023,D,9:00 AM,9:00 AM,10:00 AM,Aliber 101,1,Keynote Session,NA,Steve Jackson,Steve Jackson,TRUE,Elena Black,The Power of Back-Testing,"As a practical matter, actuaries use models in many everyday tasks, including actuarial valuations, asset liability management studies, healthcare risk scoring, pension plan re-design, and many others. One of the ways of evaluating a model is to examine how it would have worked had it been implemented in the past.  Simple in concept, but too rarely used in practice, back-testing is a powerful tool for validating a model’s strengths, but also for revealing its potential vulnerabilities.

In this presentation, Ms. Black demonstrates the insights that have been gleaned from back-testing in several real-world actuarial modeling exercises including yield curve construction, mortality improvement scales’ models, investment strategies such as liability driven investment or dynamic asset allocations, models for forecasting capital market assumptions, and healthcare risk scoring models. ","As a practical matter, actuaries use models in many everyday tasks, including actuarial valuations, asset liability management studies, healthcare risk scoring, pension plan re-design, and many others. One of the ways of evaluating a model is to examine how it would have worked had it been implemented in the past. Simple in concept, but too rarely used in practice, back-testing is a powerful tool for validating a model’s strengths, but also for revealing its potential vulnerabilities.

In this presentation, Ms. Black demonstrates the insights that have been gleaned from back-testing in several real-world actuarial modeling exercises including yield curve construction, mortality improvement scales’ models, investment strategies such as liability driven investment or dynamic asset allocations, models for forecasting capital market assumptions, and healthcare risk scoring models.",NA,Elena Black,NA,Terry Group,NA,NA,NA
8/1/2023,D,10:00 AM,10:00 AM,10:30 AM,Aliber Hall,NA,Coffee Break,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
8/1/2023,D,10:30 AM,10:30 AM,11:00 AM,Aliber 101,2,Parallel Session,(Invited Session) In Memory of Dr. Ken Seng Tan,Phelim Boyle and Chengguo Weng,Phelim Boyle and Chengguo Weng,TRUE,Shimeng Huang,Individual Loss Reserving Using Marked Point Process,"Accurate prediction of an insurer’s outstanding liabilities is crucial for maintaining the financial health of the insurance sector. We aim to develop a statistical model for insurers to dynamically forecast unpaid losses by leveraging the granular transaction data on individual claims. The liability cash flow from a single insurance claim is determined by an event process that describes the recurrences of payments, a payment process that generates a sequence of payment amounts, and a settlement process that terminates both the event and payment processes. More importantly, the three components are dependent on one another, which enables the dynamic prediction of an insurer’s outstanding liability. We introduce a copula-based point process framework to model the recurrent events of payment transactions from an insurance claim, where the longitudinal payment amounts and the time-to-settlement outcome are formulated as the marks and the terminal event of the counting process, respectively. The dependencies among the three components are characterized using the method of pair copula constructions. We further develop a stage-wise strategy for parameter estimation and illustrate its desirable properties with numerical experiments.
In the application, we consider a portfolio of property insurance claims for building and contents coverage obtained from a commercial property insurance provider, where we find interesting dependence patterns among the three components. The superior performance of dynamic prediction strategy implied by the proposed joint model enhances the insurer’s decision making in claims reserving and risk financing operations.
","Accurate prediction of an insurer’s outstanding liabilities is crucial for maintaining the financial health of the insurance sector. We aim to develop a statistical model for insurers to dynamically forecast unpaid losses by leveraging the granular transaction data on individual claims. The liability cash flow from a single insurance claim is determined by an event process that describes the recurrences of payments, a payment process that generates a sequence of payment amounts, and a settlement process that terminates both the event and payment processes. More importantly, the three components are dependent on one another, which enables the dynamic prediction of an insurer’s outstanding liability. We introduce a copula-based point process framework to model the recurrent events of payment transactions from an insurance claim, where the longitudinal payment amounts and the time-to-settlement outcome are formulated as the marks and the terminal event of the counting process, respectively. The dependencies among the three components are characterized using the method of pair copula constructions. We further develop a stage-wise strategy for parameter estimation and illustrate its desirable properties with numerical experiments.
In the application, we consider a portfolio of property insurance claims for building and contents coverage obtained from a commercial property insurance provider, where we find interesting dependence patterns among the three components. The superior performance of dynamic prediction strategy implied by the proposed joint model enhances the insurer’s decision making in claims reserving and risk financing operations.
",,Shimeng Huang,,University of Wisconsin-Madison,,,Session in memory of Dr. Ken Seng Tan
8/1/2023,D,10:30 AM,11:00 AM,11:30 AM,Aliber 101,2,Parallel Session,(Invited Session) In Memory of Dr. Ken Seng Tan,Phelim Boyle and Chengguo Weng,Phelim Boyle and Chengguo Weng,TRUE,Yanbin Xu,Borrowing Information Across Space and Time: Pricing Flood Risk With Physics-Based Hierarchical Machine Learning Models,"This paper proposes a physics-based hierarchical deep learning framework for flood risk modeling that integrates high-resolution meteorological and hydraulic data. The framework employs a hierarchical deep learning model structure based on geographical locations. Using the Mississippi River as a laboratory, we demonstrate that the proposed framework outperforms conventional deep learning model benchmarks. We also apply the model-generated risk factors to the national flood insurance program (NFIP) policy and claim dataset to test the risk pricing performance. The risk factor generated by the proposed model not only improves the net premium by better tracking underlying risks, but also lowers the solvency capital requirement. Our global optimization approach allows the model to leverage spatial and temporal information, while the physics-based hierarchical structure improves the interpretability of the deep learning models and addresses the challenges posed by anthropogenic effects in flood risk prediction.","This paper proposes a physics-based hierarchical deep learning framework for flood risk modeling that integrates high-resolution meteorological and hydraulic data. The framework employs a hierarchical deep learning model structure based on geographical locations. Using the Mississippi River as a laboratory, we demonstrate that the proposed framework outperforms conventional deep learning model benchmarks. We also apply the model-generated risk factors to the national flood insurance program (NFIP) policy and claim dataset to test the risk pricing performance. The risk factor generated by the proposed model not only improves the net premium by better tracking underlying risks, but also lowers the solvency capital requirement. Our global optimization approach allows the model to leverage spatial and temporal information, while the physics-based hierarchical structure improves the interpretability of the deep learning models and addresses the challenges posed by anthropogenic effects in flood risk prediction.",,Yanbin Xu,,Nanyang Technological University,,,Session in memory of Dr. Ken Seng Tan
8/1/2023,D,10:30 AM,11:30 AM,12:00 PM,Aliber 101,2,Parallel Session,(Invited Session) In Memory of Dr. Ken Seng Tan,Phelim Boyle and Chengguo Weng,Phelim Boyle and Chengguo Weng,TRUE,Jiayue Zhang,Navigating Uncertainty in ESG Investing,"The focus of this thesis is to investigate the application of sustainable and green investments in the finance and insurance industries, specifically addressing their relevance to addressing climate change and resiliency. To develop a portfolio optimized for both financial returns and sustainability factors, ESG scores are integrated into the reward function of a conventional Reinforcement Learning model. This approach enables a more robust analysis of the impact of various ESG ratings published by major rating agencies on the coherence of investment strategies. The model addresses the widespread confusion arising from the high heterogeneity in published ESG ratings by treating it as a source of ambiguity, and proposes four ESG ensemble strategies catering to investors with different risk and (smooth) ambiguity preference profiles. Additionally, a Double-Mean-Variance model is constructed to combine financial returns and ESG score objectives, define three investor types based on their ambiguity preferences, and develop novel ESG-modified Capital Asset Pricing Models to evaluate the resulting optimized portfolio performance. The Fama-French three-factor model is also extended to examine the individual contributions of the three ESG pillars.","The focus of this thesis is to investigate the application of sustainable and green investments in the finance and insurance industries, specifically addressing their relevance to addressing climate change and resiliency. To develop a portfolio optimized for both financial returns and sustainability factors, ESG scores are integrated into the reward function of a conventional Reinforcement Learning model. This approach enables a more robust analysis of the impact of various ESG ratings published by major rating agencies on the coherence of investment strategies. The model addresses the widespread confusion arising from the high heterogeneity in published ESG ratings by treating it as a source of ambiguity, and proposes four ESG ensemble strategies catering to investors with different risk and (smooth) ambiguity preference profiles. Additionally, a Double-Mean-Variance model is constructed to combine financial returns and ESG score objectives, define three investor types based on their ambiguity preferences, and develop novel ESG-modified Capital Asset Pricing Models to evaluate the resulting optimized portfolio performance. The Fama-French three-factor model is also extended to examine the individual contributions of the three ESG pillars.",,Jiayue Zhang,,University of Waterloo,,,Session in memory of Dr. Ken Seng Tan
8/1/2023,D,10:30 AM,10:30 AM,11:00 AM,Aliber 102,3,Parallel Session,Risk Modeling and Measurement II,Sahadeb Upretee,Sahadeb Upretee,TRUE,Nariankadu Shyamalkumar,On One-Factor Copulas Ability To Capture Multivariate Tail-Dependence,"Modeling multivariate dependence in high dimensions is challenging, with popular solutions resorting to the construction of a multivariate copula as a composition of lower dimensional copulas.  One popular parsimonious solution is factor copulas, and in particular, the one-factor copula is touted for its simplicity - with the number of parameters linear in the dimension - while being able to cater to asymmetric dependence in the tails. In this talk, we will add nuance to this claim from the point of view of a popular measure of tail dependence, the tail dependence matrix. This work is a collaboration with Prof. Siyang Tao of Ball State University.","Modeling multivariate dependence in high dimensions is challenging, with popular solutions resorting to the construction of a multivariate copula as a composition of lower dimensional copulas. One popular parsimonious solution is factor copulas, and in particular, the one-factor copula is touted for its simplicity - with the number of parameters linear in the dimension - while being able to cater to asymmetric dependence in the tails. In this talk, we will add nuance to this claim from the point of view of a popular measure of tail dependence, the tail dependence matrix. This work is a collaboration with Prof. Siyang Tao of Ball State University.",,Nariankadu Shyamalkumar, ,The University of Iowa,,,Dependence modeling
8/1/2023,D,10:30 AM,11:00 AM,11:30 AM,Aliber 102,3,Parallel Session,Risk Modeling and Measurement II,Sahadeb Upretee,Sahadeb Upretee,TRUE,Qian Zhao,Model Uncertainty and Selection of Risk Models for Left-Truncated and Right-Censored Loss Data," Insurance loss data are usually in the form of left-truncation and right-censoring (LTRC) due to deductibles and policy limits respectively. This paper investigates the model uncertainty and selection procedure when various parametric models are constructed to accommodate such LTRC data. The joint asymptotic properties of the estimators have been established using the Delta method along with Maximum Likelihood Estimation when the model is specified.  We conduct the simulation studies using Fisk, Frechet, Lognormal, Lomax, Paralogistic, and Weibull distributions with various proportions of loss data below deductibles and above policy limits. A variety of graphic tools, hypothesis tests (with bootstrapped p-values), and penalized likelihood criteria are employed to validate the models, and their performances on the model selection are thoroughly compared for each underlying distribution. When multiple models are difficult to distinguish, model averaging could be considered and the p-values of Anderson-Darling (AD) test statistics serve as a strong indicator to separate the candidates. The effect of model choice and parameter estimation method on risk pricing is also illustrated using actual data that represent Wisconsin property losses in the United States from 2007 to 2010.","Insurance loss data are usually in the form of left-truncation and right-censoring (LTRC) due to deductibles and policy limits respectively. This paper investigates the model uncertainty and selection procedure when various parametric models are constructed to accommodate such LTRC data. The joint asymptotic properties of the estimators have been established using the Delta method along with Maximum Likelihood Estimation when the model is specified. We conduct the simulation studies using Fisk, Frechet, Lognormal, Lomax, Paralogistic, and Weibull distributions with various proportions of loss data below deductibles and above policy limits. A variety of graphic tools, hypothesis tests (with bootstrapped p-values), and penalized likelihood criteria are employed to validate the models, and their performances on the model selection are thoroughly compared for each underlying distribution. When multiple models are difficult to distinguish, model averaging could be considered and the p-values of Anderson-Darling (AD) test statistics serve as a strong indicator to separate the candidates. The effect of model choice and parameter estimation method on risk pricing is also illustrated using actual data that represent Wisconsin property losses in the United States from 2007 to 2010.", ,Qian Zhao, ,Robert Morris University,,,Risk modeling and measurement III
8/1/2023,D,10:30 AM,11:30 AM,12:00 PM,Aliber 102,3,Parallel Session,Risk Modeling and Measurement II,Sahadeb Upretee,Sahadeb Upretee,TRUE,Sahadeb Upretee,Computing and Estimating Distortion Risk Measures: How To Handle Analytically Intractable Cases?," In insurance data analytics and actuarial practice, distortion risk measures are used to capture the riskiness of the distribution tail. Point and interval estimates of the risk measures are then employed to price extreme events, to develop reserves, to design risk transfer strategies, and to allocate capital. Often the computation of those estimates relies on Monte Carlo simulations, which, depending upon the complexity of the problem, can be very costly in terms of required expertise and computational time. In this article, we study analytic and numerical evaluation of distortion risk measures, with the expectation that the proposed formulas or inequalities will reduce the computational burden. Specifically, we consider several distortion risk measures––value-at-risk (VaR), conditional tail expectation (cte), proportional hazards transform (pht), Wang transform (wt), and Gini shortfall (gs)––and evaluate them when the loss severity variable follows shifted exponential, Pareto I, and shifted lognormal distributions (all chosen to have the same support), which exhibit common distributional shapes of insurance losses. For these choices of risk measures and loss models, only the VaR and cte measures always possess explicit formulas. For pht, wt, and gs, there are cases when the analytic treatment of the measure is not feasible. In the latter situations, conditions under which the measure is finite are studied rigorously. In particular, we prove several theorems that specify two-sided bounds for the analytically intractable cases. The quality of the bounds is further investigated by comparing them with numerically evaluated risk measures. Finally, a simulation study involving application of those bounds in statistical estimation of the risk measures is also provided.","In insurance data analytics and actuarial practice, distortion risk measures are used to capture the riskiness of the distribution tail. Point and interval estimates of the risk measures are then employed to price extreme events, to develop reserves, to design risk transfer strategies, and to allocate capital. Often the computation of those estimates relies on Monte Carlo simulations, which, depending upon the complexity of the problem, can be very costly in terms of required expertise and computational time. In this article, we study analytic and numerical evaluation of distortion risk measures, with the expectation that the proposed formulas or inequalities will reduce the computational burden. Specifically, we consider several distortion risk measures––value-at-risk (VaR), conditional tail expectation (cte), proportional hazards transform (pht), Wang transform (wt), and Gini shortfall (gs)––and evaluate them when the loss severity variable follows shifted exponential, Pareto I, and shifted lognormal distributions (all chosen to have the same support), which exhibit common distributional shapes of insurance losses. For these choices of risk measures and loss models, only the VaR and cte measures always possess explicit formulas. For pht, wt, and gs, there are cases when the analytic treatment of the measure is not feasible. In the latter situations, conditions under which the measure is finite are studied rigorously. In particular, we prove several theorems that specify two-sided bounds for the analytically intractable cases. The quality of the bounds is further investigated by comparing them with numerically evaluated risk measures. Finally, a simulation study involving application of those bounds in statistical estimation of the risk measures is also provided.", ,Sahadeb Upretee, ,Central Washington University,,,Risk modeling and measurement III
8/1/2023,D,10:30 AM,10:30 AM,11:00 AM,Aliber 103,4,Parallel Session,Statistical and Machine Learning II,Zhiyu Quan,Zhiyu Quan,TRUE,Ramzi Abujamra,"An Introduction to Uplift Modeling in Care Management Intervention Programs, With an Expanded Modeling Framework","Explore a general introduction to uplift modeling, a relatively newer area in machine learning which has been successfully used in retail (Wayfair etc.). The concepts are parallel to the intervention programs in health care, where we are interested in measuring the treatment effects and ultimately directing the right members to the most effective programs, driving the highest value for the programs both from a financial and health outcomes perspective. Uplift modeling can also be viewed in the context of causality where we are interested in the impact of a treatment relative to a member not receiving the treatment.  One unique component of this talk will be a discussion about expanding the framework to go beyond treatment impact and include other impacts such as cost or revenue.  This framework will be showcased using an optimization approach (knapsack) as well as with probability distributions.  A discussion about a potential implementation that expands the demo will be covered.","Explore a general introduction to uplift modeling, a relatively newer area in machine learning which has been successfully used in retail (Wayfair etc.). The concepts are parallel to the intervention programs in health care, where we are interested in measuring the treatment effects and ultimately directing the right members to the most effective programs, driving the highest value for the programs both from a financial and health outcomes perspective. Uplift modeling can also be viewed in the context of causality where we are interested in the impact of a treatment relative to a member not receiving the treatment. One unique component of this talk will be a discussion about expanding the framework to go beyond treatment impact and include other impacts such as cost or revenue. This framework will be showcased using an optimization approach (knapsack) as well as with probability distributions. A discussion about a potential implementation that expands the demo will be covered.",,Ramzi Abujamra,,Cohere Health (Clinical Programs and Analytics),,,
8/1/2023,D,10:30 AM,11:00 AM,11:30 AM,Aliber 103,4,Parallel Session,Statistical and Machine Learning II,Zhiyu Quan,Zhiyu Quan,TRUE,Lu Xiong,Stacking Ensemble Learning for Enhancement of Insurance Regulatory Information System," The solvency of an insurance company refers to its ability to pay its debts, including insurance premiums and indemnities. The National Association of Insurance Commissioners (NAIC) developed the Insurance Regulatory Information System (IRIS) to help state insurance departments identify companies that may need regulatory attention regarding solvency. IRIS consists of 13 ratios grouped into four categories. If a company exceeds its usual range in more than four ratios, regulators will take notice. In this project, our goal is to predict whether a company will have four or more unusual IRIS ratios in the first, third, and fifth years, which is represented by being solvent or insolvent. For this, the H2O AutoML was used to classify companies as solvent or insolvent. This model uses a variety of algorithms to analyze data, select features, and tune parameters. After comparison of several models including the individual models, base ensemble models, and stacked ensemble models, the best model identified as StackedEnsemble_BestOfFamily, achieved an accuracy of 90.84%, an AUC of 0.96, and an AUPCR of 0.98, indicating that it can accurately predict insolvency in 90.84% of cases. Together with being highly accurate, this model is simple to use as it can be implemented by professionals who have minimal machine learning skills.","The solvency of an insurance company refers to its ability to pay its debts, including insurance premiums and indemnities. The National Association of Insurance Commissioners (NAIC) developed the Insurance Regulatory Information System (IRIS) to help state insurance departments identify companies that may need regulatory attention regarding solvency. IRIS consists of 13 ratios grouped into four categories. If a company exceeds its usual range in more than four ratios, regulators will take notice. In this project, our goal is to predict whether a company will have four or more unusual IRIS ratios in the first, third, and fifth years, which is represented by being solvent or insolvent. For this, the H2O AutoML was used to classify companies as solvent or insolvent. This model uses a variety of algorithms to analyze data, select features, and tune parameters. After comparison of several models including the individual models, base ensemble models, and stacked ensemble models, the best model identified as StackedEnsemble_BestOfFamily, achieved an accuracy of 90.84%, an AUC of 0.96, and an AUPCR of 0.98, indicating that it can accurately predict insolvency in 90.84% of cases. Together with being highly accurate, this model is simple to use as it can be implemented by professionals who have minimal machine learning skills.", ,Lu Xiong, ,Middle Tennessee State University,,,Statistical and machine learning I
8/1/2023,D,10:30 AM,11:30 AM,12:00 PM,Aliber 103,4,Parallel Session,Statistical and Machine Learning II,Zhiyu Quan,Zhiyu Quan,TRUE,Zhiyu Quan,On Hybrid Tree-Based Methods for Short-Term Insurance Claims," Two-part framework and the Tweedie generalized linear model (GLM) have traditionally been used to model loss costs for short-term insurance contracts. For most portfolios of insurance claims, there is typically a large proportion of zero claims that leads to imbalances, resulting in lower prediction accuracy of these traditional approaches. In this article, we propose the use of tree-based methods with a hybrid structure that involves a two-step algorithm as an alternative approach. For example, the first step is the construction of a classification tree to build the probability model for claim frequency. The second step is the application of elastic net regression models at each terminal node from the classification tree to build the distribution models for claim severity. This hybrid structure captures the benefits of tuning hyperparameters at each step of the algorithm; this allows for improved prediction accuracy, and tuning can be performed to meet specific business objectives. An obvious major advantage of this hybrid structure is improved model interpretability. We examine and compare the predictive performance of this hybrid structure relative to the traditional Tweedie GLM using both simulated and real datasets. Our empirical results show that these hybrid tree-based methods produce more accurate and informative predictions.","Two-part framework and the Tweedie generalized linear model (GLM) have traditionally been used to model loss costs for short-term insurance contracts. For most portfolios of insurance claims, there is typically a large proportion of zero claims that leads to imbalances, resulting in lower prediction accuracy of these traditional approaches. In this article, we propose the use of tree-based methods with a hybrid structure that involves a two-step algorithm as an alternative approach. For example, the first step is the construction of a classification tree to build the probability model for claim frequency. The second step is the application of elastic net regression models at each terminal node from the classification tree to build the distribution models for claim severity. This hybrid structure captures the benefits of tuning hyperparameters at each step of the algorithm; this allows for improved prediction accuracy, and tuning can be performed to meet specific business objectives. An obvious major advantage of this hybrid structure is improved model interpretability. We examine and compare the predictive performance of this hybrid structure relative to the traditional Tweedie GLM using both simulated and real datasets. Our empirical results show that these hybrid tree-based methods produce more accurate and informative predictions.", ,Zhiyu Quan, ,University of Illinois at Urbana-Champaign,,,Statistical and machine learning III
8/1/2023,D,10:30 AM,10:30 AM,11:00 AM,Aliber 107,5,Parallel Session,Mortality and Longevity Modeling I,Brian Hartman,Brian Hartman,TRUE,Yiping Guo,Fast Estimation of the Renshaw-Haberman Model and Its Variants," In mortality modelling, cohort effects are often taken into consideration as they add insights about variations in mortality across different generations. Statistically speaking, models such as the Renshaw-Haberman model may provide a better fit to historical data compared to their counterparts that incorporate no cohort effects. However, when such models are estimated using an iterative maximum likelihood method in which parameters are updated one at a time, convergence is typically slow and may not even be reached within a reasonably established maximum number of iterations. Among others, the slow convergence problem hinders the study of parameter uncertainty through bootstrapping methods. In this paper, we propose an intuitive estimation method that minimizes the sum of squared errors between actual and fitted log central death rates. The complications arising from the incorporation of cohort effects are overcome by formulating part of the optimization as a principal component analysis with missing values. Using mortality data from England and Wales, we demonstrate that our proposed method produces satisfactory estimation results and is significantly more efficient compared to the traditional likelihood-based approach.","In mortality modelling, cohort effects are often taken into consideration as they add insights about variations in mortality across different generations. Statistically speaking, models such as the Renshaw-Haberman model may provide a better fit to historical data compared to their counterparts that incorporate no cohort effects. However, when such models are estimated using an iterative maximum likelihood method in which parameters are updated one at a time, convergence is typically slow and may not even be reached within a reasonably established maximum number of iterations. Among others, the slow convergence problem hinders the study of parameter uncertainty through bootstrapping methods. In this paper, we propose an intuitive estimation method that minimizes the sum of squared errors between actual and fitted log central death rates. The complications arising from the incorporation of cohort effects are overcome by formulating part of the optimization as a principal component analysis with missing values. Using mortality data from England and Wales, we demonstrate that our proposed method produces satisfactory estimation results and is significantly more efficient compared to the traditional likelihood-based approach.", ,Yiping Guo, ,University of Waterloo,,,Mortality and longevity modeling II
8/1/2023,D,10:30 AM,11:00 AM,11:30 AM,Aliber 107,5,Parallel Session,Mortality and Longevity Modeling I,Brian Hartman,Brian Hartman,TRUE,Priscilla Mansah Codjoe and Siegfried Kaful Anyomi,Optimizing Pooled Annuity Portfolios via Adaptive Volatility Strategies," Pooled retirement plans address the annuity puzzle by providing a collective approach to managing longevity risk and offering participants the benefits of guaranteed lifetime income. Therefore, appropriately modeling unanticipated mortality improvements within these plans ensure funding adequacy. We implement mean reversion stochastic models for both mortality and interest rate to assess the potential impact of unexpected variations in mortality and interest rates. We show that by combining a dynamic target volatility strategy, such group plans can optimize portfolio risk exposure and potential returns in response to changing mortality and interest rates. ","Pooled retirement plans address the annuity puzzle by providing a collective approach to managing longevity risk and offering participants the benefits of guaranteed lifetime income. Therefore, appropriately modeling unanticipated mortality improvements within these plans ensure funding adequacy. We implement mean reversion stochastic models for both mortality and interest rate to assess the potential impact of unexpected variations in mortality and interest rates. We show that by combining a dynamic target volatility strategy, such group plans can optimize portfolio risk exposure and potential returns in response to changing mortality and interest rates.", ,Priscilla Mansah Codjoe,Siegfried Kaful Anyomi,Southern Illinois University Edwardsville,University of Iowa,,Mortality and longevity modeling II
8/1/2023,D,10:30 AM,11:30 AM,12:00 PM,Aliber 107,5,Parallel Session,Mortality and Longevity Modeling I,Brian Hartman,Brian Hartman,TRUE,Brian Hartman,A Multivariate Spatiotemporal Model for County Level Mortality Data in the Contiguous United States," Using a number of modern predictive modeling methods, we seek to understand the factors that drive mortality in the contiguous United States. The mortality data we use is indexed by county and year as well as grouped into 18 different age bins. We propose a model that adds two important contributions to existing mortality studies. First, instead of building mortality models separately by age or treating age as a fixed covariate, we treat age as a random effect. This is an improvement over previous models because it allows the model in one age group to borrow strength and information from other age groups that are nearby. The result is a multivariate spatiotemporal model and is estimated using Integrated Nested Laplace Approximations (INLA).  Second, we utilize Gaussian Processes to create nonlinear covariate effects for predictors such as unemployment rate, race, and education level.  This allows for a more flexible relationship to be modeled between mortality and these important predictors. Understanding that the United States is expansive and diverse, we also allow for many of these effects to vary by location. The amount of flexibility of our model in how predictors relate to mortality has not been used in previous mortality studies and will result in a more accurate model and a more complete understanding of the factors that drive mortality. Both the multivariate nature of the model as well as the non-linear predictors that have an interaction with space will advance the study of mortality beyond what has been done previously and will allow us to better examine the often complicated relationships between the predictors and mortality in different regions.","Using a number of modern predictive modeling methods, we seek to understand the factors that drive mortality in the contiguous United States. The mortality data we use is indexed by county and year as well as grouped into 18 different age bins. We propose a model that adds two important contributions to existing mortality studies. First, instead of building mortality models separately by age or treating age as a fixed covariate, we treat age as a random effect. This is an improvement over previous models because it allows the model in one age group to borrow strength and information from other age groups that are nearby. The result is a multivariate spatiotemporal model and is estimated using Integrated Nested Laplace Approximations (INLA). Second, we utilize Gaussian Processes to create nonlinear covariate effects for predictors such as unemployment rate, race, and education level. This allows for a more flexible relationship to be modeled between mortality and these important predictors. Understanding that the United States is expansive and diverse, we also allow for many of these effects to vary by location. The amount of flexibility of our model in how predictors relate to mortality has not been used in previous mortality studies and will result in a more accurate model and a more complete understanding of the factors that drive mortality. Both the multivariate nature of the model as well as the non-linear predictors that have an interaction with space will advance the study of mortality beyond what has been done previously and will allow us to better examine the often complicated relationships between the predictors and mortality in different regions.", ,Brian Hartman, ,Brigham Young University,,,Mortality and longevity modeling II
8/1/2023,E,12:00 PM,12:00 PM,1:30 PM,Hubbell Dining Hall,NA,Lunch,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
8/1/2023,E,1:30 PM,1:30 PM,3:00 PM,Aliber 101,1,Parallel Session,(Invited Session) DE&I Panel,NA,NA,NA,NA,"Is the Workplace Leading or Following Society in Diversity, Equity, and Inclusion?","Diversity, equity, and inclusion is a critical component of both a just society and workplace.  This session brings together thought leaders and practitioners with experience in the insurance industry to share their goals, successes, challenges, and measurable outcomes.  Learn how they identified the appropriate next steps for their organizations, how they determined whether success is defined as a milestone or a finish line, what expected and unforeseen challenges occurred and how they pivoted to improve, and if improvement can be measured.  Panelists have experience in both formulating strategies and executing plans. ","Diversity, equity, and inclusion is a critical component of both a just society and workplace. This session brings together thought leaders and practitioners with experience in the insurance industry to share their goals, successes, challenges, and measurable outcomes. Learn how they identified the appropriate next steps for their organizations, how they determined whether success is defined as a milestone or a finish line, what expected and unforeseen challenges occurred and how they pivoted to improve, and if improvement can be measured. Panelists have experience in both formulating strategies and executing plans.",,NA,,NA,,,DE&I
8/1/2023,E,1:30 PM,1:30 PM,2:00 PM,Aliber 102,2,Parallel Session,Optimal Control and Optimization II,Wenyuan Li,Wenyuan Li,TRUE,Jiarui Yu,What Benefits Drive Membership in Medicare Advantage Plans?,"Medicare Advantage plans offer a wide range of benefits ranging from financial (e.g., deductible) to ancillary (e.g., meals, transports). We use enrollment, performance, and benefits data for Connecticut 2019-2022 to identify the most relevant benefits that drive membership. We apply the Non-Negative Matrix Factorization algorithm to reduce dimensionality and summarize the benefit features present in the extracted data while obtaining a set of indices that summarize the structure of benefits. Then, we employ a linear mixed model and linear projection to quantify the correlation between extracted features and plan enrollment. We identify the correlation among the market share and inpatient copay days, vision exams, remote access technologies, health education, etc. The model also shows that brand has a statistically significant effect and indirect impact of star ratings on membership.","Medicare Advantage plans offer a wide range of benefits ranging from financial (e.g., deductible) to ancillary (e.g., meals, transports). We use enrollment, performance, and benefits data for Connecticut 2019-2022 to identify the most relevant benefits that drive membership. We apply the Non-Negative Matrix Factorization algorithm to reduce dimensionality and summarize the benefit features present in the extracted data while obtaining a set of indices that summarize the structure of benefits. Then, we employ a linear mixed model and linear projection to quantify the correlation between extracted features and plan enrollment. We identify the correlation among the market share and inpatient copay days, vision exams, remote access technologies, health education, etc. The model also shows that brand has a statistically significant effect and indirect impact of star ratings on membership.",,Jiarui Yu, ,UCSB,,,Insurance economics I
8/1/2023,E,1:30 PM,2:00 PM,2:30 PM,Aliber 102,2,Parallel Session,Optimal Control and Optimization II,Wenyuan Li,Wenyuan Li,TRUE,Wei Wei,Optimal Insurance Design Under Smooth Ambiguity,"We analyze optimal insurance design for a risk- and ambiguity-averse policyholder who is uncertain about the distribution of losses and faces linear transaction costs. We use smooth ambiguity preferences, a flexible ambiguity structure, and focus on indemnity schedules that satisfy the principle of indemnity and the no-sabotage condition for incentive compatibility. We characterize optimal insurance contracts and find that the marginal indemnity is either zero or one except at critical points. We then provide a condition for a straight deductible to be optimal and show that this condition is satisfied under various stochastic ordering assumptions on the priors. We discuss specific ambiguity structures, some of which give rise to indemnities with multiple layers. We also derive comparative statics. Greater ambiguity aversion always raises insurance demand whereas greater ambiguity has indeterminate effects. For policyholders with relative ambiguity prudence between zero and two, greater ambiguity raises insurance demand.","We analyze optimal insurance design for a risk- and ambiguity-averse policyholder who is uncertain about the distribution of losses and faces linear transaction costs. We use smooth ambiguity preferences, a flexible ambiguity structure, and focus on indemnity schedules that satisfy the principle of indemnity and the no-sabotage condition for incentive compatibility. We characterize optimal insurance contracts and find that the marginal indemnity is either zero or one except at critical points. We then provide a condition for a straight deductible to be optimal and show that this condition is satisfied under various stochastic ordering assumptions on the priors. We discuss specific ambiguity structures, some of which give rise to indemnities with multiple layers. We also derive comparative statics. Greater ambiguity aversion always raises insurance demand whereas greater ambiguity has indeterminate effects. For policyholders with relative ambiguity prudence between zero and two, greater ambiguity raises insurance demand.",,Wei Wei, ,University of Illinois Urbana-Champaign,,,Insurance economics I
8/1/2023,E,1:30 PM,2:30 PM,3:00 PM,Aliber 102,2,Parallel Session,Optimal Control and Optimization II,Wenyuan Li,Wenyuan Li,TRUE,Wenyuan Li,Constrained Portfolio Optimization in a Life-Cycle Model,"In this paper, we consider an individual with a stochastic income manages a portfolio consisting of stocks, a bond, and life insurance to maximize his or her consumption level, death benefit, and terminal wealth. Meanwhile, the individual faces a convex-set trading constraint, of which the non-tradeable asset constraint, no short-selling constraint, and no borrowing constraint are special cases. Following Cuoco (1997), we build the artificial markets to derive the dual problem and prove the existence of the original problem. With additional discussions, we extend his uniformly bounded assumption on the interest rate to an almost surely finite expectation condition and enlarge his uniformly bounded assumption on the income process to a bounded expectation condition. Moreover, we propose a dual control neural network approach to compute tight lower and upper bounds for the original problem, which can be utilized in more general cases than the simulation of artificial markets strategies (SAMS) approach in Bick et al. (2013). Finally, we conclude that when considering the trading constraints, the individual will reduce his or her demand for life insurance.","In this paper, we consider an individual with a stochastic income manages a portfolio consisting of stocks, a bond, and life insurance to maximize his or her consumption level, death benefit, and terminal wealth. Meanwhile, the individual faces a convex-set trading constraint, of which the non-tradeable asset constraint, no short-selling constraint, and no borrowing constraint are special cases. Following Cuoco (1997), we build the artificial markets to derive the dual problem and prove the existence of the original problem. With additional discussions, we extend his uniformly bounded assumption on the interest rate to an almost surely finite expectation condition and enlarge his uniformly bounded assumption on the income process to a bounded expectation condition. Moreover, we propose a dual control neural network approach to compute tight lower and upper bounds for the original problem, which can be utilized in more general cases than the simulation of artificial markets strategies (SAMS) approach in Bick et al. (2013). Finally, we conclude that when considering the trading constraints, the individual will reduce his or her demand for life insurance.",,Wenyuan Li, ,The University of Hong Kong,,,Insurance economics I
8/1/2023,E,1:30 PM,1:30 PM,2:00 PM,Aliber 103,3,Parallel Session,"Blockchain, Telematics, and InsurTech II",Arnold Shapiro,Arnold Shapiro,TRUE,Jiajie Yang,Distributed Insurance: Single and Multi-Period,"This article explores the problem of multi-period premiums and loss allocation in distributed insurance. In recent years, various platforms like Nexus Mutual and Etherisc have opened up the insurance market to individual investors, allowing them to act as insurers by providing capital, undertaking risks, and collecting premiums. These platforms have broadened investment opportunities for the general public. However, there has been limited theory behind the practice. In this paper, we begin by studying the distinction between two specific loss allocations and develop a set of viable non-proportional loss allocation strategies for the one-period scenario. Moving on to the multi-period problem, we propose a novel loss allocation approach that promotes stability in our insurance loss-sharing system. Additionally, we present some non-proportional premium allocations aimed at fostering business expansion in distributed insurance. The article concludes with a numerical example.","This article explores the problem of multi-period premiums and loss allocation in distributed insurance. In recent years, various platforms like Nexus Mutual and Etherisc have opened up the insurance market to individual investors, allowing them to act as insurers by providing capital, undertaking risks, and collecting premiums. These platforms have broadened investment opportunities for the general public. However, there has been limited theory behind the practice. In this paper, we begin by studying the distinction between two specific loss allocations and develop a set of viable non-proportional loss allocation strategies for the one-period scenario. Moving on to the multi-period problem, we propose a novel loss allocation approach that promotes stability in our insurance loss-sharing system. Additionally, we present some non-proportional premium allocations aimed at fostering business expansion in distributed insurance. The article concludes with a numerical example.",,Jiajie Yang, ,University of Illinois Urbana-Champaign,,,"Blockchain, Telematics, and InsurTech I"
8/1/2023,E,1:30 PM,2:00 PM,2:30 PM,Aliber 103,3,Parallel Session,"Blockchain, Telematics, and InsurTech II",Arnold Shapiro,Arnold Shapiro,TRUE,Hashan Savinda Peiris Kalugama Gardige,Integration of Traditional and Telematics Data for Efficient Insurance Claims Prediction,"While driver telematics has gained attention for risk classification in auto insurance, scarcity of observations with telematics features has been problematic, which could be owing to either privacy concern or adverse selection compared to the data points with traditional features.
 
To handle this issue, we propose a data integration technique based on calibration weights. It is shown that the proposed technique can efficiently integrate the so-called traditional data and telematics data and also cope with possible adverse selection issues on the availability of telematics data. Our findings are supported by a simulation study and empirical analysis on a synthetic telematics data set.","While driver telematics has gained attention for risk classification in auto insurance, scarcity of observations with telematics features has been problematic, which could be owing to either privacy concern or adverse selection compared to the data points with traditional features.
 
To handle this issue, we propose a data integration technique based on calibration weights. It is shown that the proposed technique can efficiently integrate the so-called traditional data and telematics data and also cope with possible adverse selection issues on the availability of telematics data. Our findings are supported by a simulation study and empirical analysis on a synthetic telematics data set.",,Hashan Savinda Peiris Kalugama Gardige, ,Simon Fraser University,,,"Blockchain, Telematics, and InsurTech I"
8/1/2023,E,1:30 PM,2:30 PM,3:00 PM,Aliber 103,3,Parallel Session,"Blockchain, Telematics, and InsurTech II",Arnold Shapiro,Arnold Shapiro,TRUE,Arnold Shapiro,Blockchain Tontines,"At ARC 2018, I presented a talk on blockchains, which dealt with what they are, how they worked, and their implications for the insurance industry.  A primary goal of that presentation was to convey the notion that a blockchain was a decentralized transaction and data management technology, which featured a distributed, immutable digital record system that was shared among many independent parties and could be updated only by their consensus.
 
This presentation extends that discussion by exploring the implementation of blockchain tontines.  We begin with a brief overview of tontines.  The topics discussed include an intuitive justification of essential tontine equations and tontine flowcharts. We then turn to the central theme of this presentation, the role of blockchains and smart contracts in the development of secure and transparent tontines.  We conclude with a commentary on applications of blockchain tontines.","At ARC 2018, I presented a talk on blockchains, which dealt with what they are, how they worked, and their implications for the insurance industry. A primary goal of that presentation was to convey the notion that a blockchain was a decentralized transaction and data management technology, which featured a distributed, immutable digital record system that was shared among many independent parties and could be updated only by their consensus.
 
This presentation extends that discussion by exploring the implementation of blockchain tontines. We begin with a brief overview of tontines. The topics discussed include an intuitive justification of essential tontine equations and tontine flowcharts. We then turn to the central theme of this presentation, the role of blockchains and smart contracts in the development of secure and transparent tontines. We conclude with a commentary on applications of blockchain tontines.",,Arnold Shapiro, ,Penn State University,,,"Blockchain, Telematics, and InsurTech I"
8/1/2023,E,1:30 PM,1:30 PM,2:00 PM,Aliber 107,4,Parallel Session,Life Insurance,Junsen Tang,Junsen Tang,TRUE,Yuanyuan Zhang,Dynamic Robust Longevity Risk Management,"This paper examines the optimal design and management of reinsurance for longevity risk in the presence of ambiguity surrounding the future mortality experience of the risk exposure holder. Using a Stackelberg differential game approach between a reinsurer and the longevity risk exposure holder, we analyze the conditions under which different types of contracts can enable the existence of a market. Additionally, we provide semi-explicit solutions for the optimal indemnity rates and reinsurance premiums when a market does exist. Our findings indicate that ambiguity aversion towards longevity risk poses a significant obstacle to the development of the longevity risk transfer market. To address this issue, we propose a dynamic longevity reinsurance contract with time-varying indemnity rates, which can enhance the expected utility of both the reinsurer and the exposure holder compared to traditional reinsurance with a static indemnity rate. This dynamic contract design can facilitate transactions of longevity reinsurance, ultimately leading to the growth of the longevity risk transfer market.","This paper examines the optimal design and management of reinsurance for longevity risk in the presence of ambiguity surrounding the future mortality experience of the risk exposure holder. Using a Stackelberg differential game approach between a reinsurer and the longevity risk exposure holder, we analyze the conditions under which different types of contracts can enable the existence of a market. Additionally, we provide semi-explicit solutions for the optimal indemnity rates and reinsurance premiums when a market does exist. Our findings indicate that ambiguity aversion towards longevity risk poses a significant obstacle to the development of the longevity risk transfer market. To address this issue, we propose a dynamic longevity reinsurance contract with time-varying indemnity rates, which can enhance the expected utility of both the reinsurer and the exposure holder compared to traditional reinsurance with a static indemnity rate. This dynamic contract design can facilitate transactions of longevity reinsurance, ultimately leading to the growth of the longevity risk transfer market.", ,Yuanyuan Zhang, ,University of Waterloo,,,Life Insurance
8/1/2023,E,1:30 PM,2:00 PM,2:30 PM,Aliber 107,4,Parallel Session,Life Insurance,Junsen Tang,Junsen Tang,TRUE,Wenyi Lu,Verification of Jewell’s Inequality vs Equivalence Principle via Graphs of Gain Functions," This presentation provides a visualizable and tangible version of verifying the surprising puzzle stated by the late Professor William S. Jewell that, for a fully continuous whole life insurance policy with level premiums and a constant death benefit, even though the equivalence principle stipulates that the insurer’s expected gain at issue is zero, the insurer’s expected gain at the moment of death of the insured is positive. This puzzle/fact is named Jewell’s inequality by Professor Hans Gerber and Professor Elias Shiu.

The author demonstrates via graphs that the curve of the Gain of the insurer at issue is concave-down and the counter curve of the Gain of the insurer at expiry is convex-up. Per the property of Jensen’s inequality, the author then justifies that the premium determined by Equivalence Principle (expected break-even at issue) is for sure larger than the premium determined by expected break-even at expiry. This is equivalent to confirming Jewell’s inequality.

In addition, in the sense of financial interpretation, the author justifies the aforementioned puzzle/fact with the concept of probability of sufficiency of level benefit premiums, together with certainty vs uncertainty of the time of expected break-even. Specifically, with the liabilities being defined in the policies already, the more premiums the insurer can charge, the more financial security the insurer has on this line of business. Conversely, an articulate argument is that the equivalence principle requires an expected break-even at issuance (time 0, which is fixed), whereas the expected gain of 0 at expiry requires an expected break-even at expiry time, an uncertain time, a random variable. The first case clearly indicates more financial security to the insurer than the latter case does.

Moreover, the author verifies Jewell’s inequality for n-payment-year whole life insurance via covariance inequality, using the technique mentioned by Professor Hans Gerber and Professor Elias Shiu.","This presentation provides a visualizable and tangible version of verifying the surprising puzzle stated by the late Professor William S. Jewell that, for a fully continuous whole life insurance policy with level premiums and a constant death benefit, even though the equivalence principle stipulates that the insurer’s expected gain at issue is zero, the insurer’s expected gain at the moment of death of the insured is positive. This puzzle/fact is named Jewell’s inequality by Professor Hans Gerber and Professor Elias Shiu.

The author demonstrates via graphs that the curve of the Gain of the insurer at issue is concave-down and the counter curve of the Gain of the insurer at expiry is convex-up. Per the property of Jensen’s inequality, the author then justifies that the premium determined by Equivalence Principle (expected break-even at issue) is for sure larger than the premium determined by expected break-even at expiry. This is equivalent to confirming Jewell’s inequality.

In addition, in the sense of financial interpretation, the author justifies the aforementioned puzzle/fact with the concept of probability of sufficiency of level benefit premiums, together with certainty vs uncertainty of the time of expected break-even. Specifically, with the liabilities being defined in the policies already, the more premiums the insurer can charge, the more financial security the insurer has on this line of business. Conversely, an articulate argument is that the equivalence principle requires an expected break-even at issuance (time 0, which is fixed), whereas the expected gain of 0 at expiry requires an expected break-even at expiry time, an uncertain time, a random variable. The first case clearly indicates more financial security to the insurer than the latter case does.

Moreover, the author verifies Jewell’s inequality for n-payment-year whole life insurance via covariance inequality, using the technique mentioned by Professor Hans Gerber and Professor Elias Shiu.", ,Wenyi Lu, ,The University of Texas at Dallas,,,Life Insurance
8/1/2023,E,1:30 PM,2:30 PM,3:00 PM,Aliber 107,4,Parallel Session,Life Insurance,Junsen Tang,Junsen Tang,TRUE,Junsen Tang,On the Actuarial Fairness and Marketability of Reverse Mortgages With Tenure Payment Option: The Optimal Surrender Policy Perspective,"This study conducts an optimal surrender analysis of reverse mortgage (RM) loans offered to elderly homeowners as a financing option. Recent market evidence on borrower early surrenders has raised concerns about the marketability of RM products and their impact on the program viability. In this paper, we derive the borrower optimal surrender strategy as a function of the underlying value of the home used as collateral for RM contracts with tenure payment option. Using a probabilistic approach to American option pricing, we present a decomposition result for the value of the contract as the sum of its European counterpart without the surrendering provision and an early exercise premium. By means of the resulting surrender boundary and reference probabilities, we explain the existing market evidence about borrower rational lapse and propose an asset–liability matching rule for valuation fairness and marketability of the prevalent RM programs.","This study conducts an optimal surrender analysis of reverse mortgage (RM) loans offered to elderly homeowners as a financing option. Recent market evidence on borrower early surrenders has raised concerns about the marketability of RM products and their impact on the program viability. In this paper, we derive the borrower optimal surrender strategy as a function of the underlying value of the home used as collateral for RM contracts with tenure payment option. Using a probabilistic approach to American option pricing, we present a decomposition result for the value of the contract as the sum of its European counterpart without the surrendering provision and an early exercise premium. By means of the resulting surrender boundary and reference probabilities, we explain the existing market evidence about borrower rational lapse and propose an asset–liability matching rule for valuation fairness and marketability of the prevalent RM programs.", ,Junsen Tang, ,University of Saint Thomas,,,Life Insurance
8/1/2023,E,1:30 PM,1:30 PM,2:00 PM,Aliber 108,5,Parallel Session,Catastrophic Risk Modeling,Fan Yang,Fan Yang,TRUE,Vajira Manathunga,Pandemic Bonds and Stochastic Logistic Growth Model,"Pandemic bonds can be used as an effective tool to mitigate economic losses governments confronts during pandemic and transfer it to global capital market. Once considered ``uninsurable,"" pandemic bonds came to world attention with the World Bank pandemic bonds issuance in 2017. Compared to other CAT bonds, pandemic bonds received less attention from actuaries, industry professionals and academic researchers. Existing research mostly focused on how to bring epidemiological parameters to the pricing mechanism through compartmental models. In this research, we introduce stochastic logistic growth model based pandemic bond pricing framework. We demonstrate proposed model by calibrating it using COVID-19 data and pricing zero coupon bond. The model can be used as an alternative to epidemic compartmental model based pandemic bond pricing mechanisms.","Pandemic bonds can be used as an effective tool to mitigate economic losses governments confronts during pandemic and transfer it to global capital market. Once considered ``uninsurable,"" pandemic bonds came to world attention with the World Bank pandemic bonds issuance in 2017. Compared to other CAT bonds, pandemic bonds received less attention from actuaries, industry professionals and academic researchers. Existing research mostly focused on how to bring epidemiological parameters to the pricing mechanism through compartmental models. In this research, we introduce stochastic logistic growth model based pandemic bond pricing framework. We demonstrate proposed model by calibrating it using COVID-19 data and pricing zero coupon bond. The model can be used as an alternative to epidemic compartmental model based pandemic bond pricing mechanisms.",,Vajira Manathunga, ,Middle Tennessee State University,,,Catastrophic risk modeling
8/1/2023,E,1:30 PM,2:00 PM,2:30 PM,Aliber 108,5,Parallel Session,Catastrophic Risk Modeling,Fan Yang,Fan Yang,TRUE,Natalia Humphreys,Exceedance Probability in Catastrophe Modeling,"In this presentation we explore two of the most important notions in Catastrophe Modeling: the Occurrence Exceedance Probability (OEP) and the Aggregate Exceedance Probability (AEP) curves.  Construction of each curve is discussed and comparisons are made.   Several numerical and theoretical examples demonstrate introduced metrics and techniques.   A separate discussion is dedicated to a connection between the distribution of loss severities and the OEP depending on the distribution of claim counts.  The presentation is concluded with demonstration of OEP and AEP curves for the deadliest, costliest, and most intense US tropical cyclones based on the 2011 National Oceanic and Atmospheric Administration (NOAA) report.","In this presentation we explore two of the most important notions in Catastrophe Modeling: the Occurrence Exceedance Probability (OEP) and the Aggregate Exceedance Probability (AEP) curves. Construction of each curve is discussed and comparisons are made. Several numerical and theoretical examples demonstrate introduced metrics and techniques. A separate discussion is dedicated to a connection between the distribution of loss severities and the OEP depending on the distribution of claim counts. The presentation is concluded with demonstration of OEP and AEP curves for the deadliest, costliest, and most intense US tropical cyclones based on the 2011 National Oceanic and Atmospheric Administration (NOAA) report.",,Natalia Humphreys, ,University of Texas at Dallas,,,Catastrophic risk modeling
8/1/2023,E,1:30 PM,2:30 PM,3:00 PM,Aliber 108,5,Parallel Session,Catastrophic Risk Modeling,Fan Yang,Fan Yang,TRUE,Fan Yang,Catastrophe Risk Pooling,"The ongoing climate change causes natural disaster losses are on the rise. The huge gap between actual and insured losses, however, is growing. Insurance-based solutions are receiving increasing attention to mitigate disaster and climate risks. Due to the features of catastrophic risks, catastrophe insurance market has failed to provide sufficient protections. In this paper we discuss how pooling catastrophe risks from various geographies and perils can be an effective tool for diversifying catastrophic risks and reducing risk premiums. Simulations and real data analysis are provided to illustrate our results.","The ongoing climate change causes natural disaster losses are on the rise. The huge gap between actual and insured losses, however, is growing. Insurance-based solutions are receiving increasing attention to mitigate disaster and climate risks. Due to the features of catastrophic risks, catastrophe insurance market has failed to provide sufficient protections. In this paper we discuss how pooling catastrophe risks from various geographies and perils can be an effective tool for diversifying catastrophic risks and reducing risk premiums. Simulations and real data analysis are provided to illustrate our results.",,Fan Yang, ,University of Waterloo,,,Catastrophic risk modeling
8/1/2023,F,3:00 PM,3:00 PM,3:20 PM,Aliber Hall,NA,Coffee Break,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
8/1/2023,F,3:20 PM,3:20 PM,3:50 PM,Aliber 101,1,Parallel Session,"(Invited Session) Education Updates from CIA, CAS, and SOA",Stuart Klugman,Stuart Klugman,TRUE,Alicia Rollo,New Options and Opportunities in the CIA Education System," Learn how the Canadian Institute of Actuaries is transforming actuarial education and the qualification process in Canada to provide innovative, flexible, and accessible options for all candidates. Through an expansion of our 10+ year university accreditation program across 11 accredited universities, and a new suite of modern educational products leading to qualification as ACIA or FCIA, the future of actuarial qualification in Canada is bright!","Learn how the Canadian Institute of Actuaries is transforming actuarial education and the qualification process in Canada to provide innovative, flexible, and accessible options for all candidates. Through an expansion of our 10+ year university accreditation program across 11 accredited universities, and a new suite of modern educational products leading to qualification as ACIA or FCIA, the future of actuarial qualification in Canada is bright!", ,Alicia Rollo, ,Canadian Institute of Actuaries, , ," Education Updates from CIA, CAS, and SOA"
8/1/2023,F,3:20 PM,3:50 PM,4:20 PM,Aliber 101,1,Parallel Session,"(Invited Session) Education Updates from CIA, CAS, and SOA",Stuart Klugman,Stuart Klugman,TRUE,Ken Williams and Alisa Walch ,CAS Resources To Help Prepare Your Students To Be the Next Generation of Property/Casualty Actuaries," Looking to learn about the exciting new updates and resources being offered by the Casualty Actuarial Society? Attend this session that will cover the recent changes to the CAS credentialing pathway, as well as resources offered to students and academics to gain more exposure to the property and casualty insurance industry. Additional highlights include student opportunities like the CAS Student Central Summer Program and the various off-the-shelf resources for academics such as CAS case studies and competition toolkits.  CAS staff and volunteers who work first-hand on these various initiatives will answer your questions and hear your feedback throughout the session!","Looking to learn about the exciting new updates and resources being offered by the Casualty Actuarial Society? Attend this session that will cover the recent changes to the CAS credentialing pathway, as well as resources offered to students and academics to gain more exposure to the property and casualty insurance industry. Additional highlights include student opportunities like the CAS Student Central Summer Program and the various off-the-shelf resources for academics such as CAS case studies and competition toolkits. CAS staff and volunteers who work first-hand on these various initiatives will answer your questions and hear your feedback throughout the session!", ,Ken Williams,Alisa Walch ,Casualty Actuarial Society,Casualty Actuarial Society, ," Education Updates from CIA, CAS, and SOA"
8/1/2023,F,3:20 PM,4:20 PM,4:50 PM,Aliber 101,1,Parallel Session,"(Invited Session) Education Updates from CIA, CAS, and SOA",Stuart Klugman,Stuart Klugman,TRUE,Stuart Klugman,SOA Education Updates, This annual update will provide a review of SOA education and examination over the past year and provide a preview of upcoming changes. There will be time for attendees to ask questions.,This annual update will provide a review of SOA education and examination over the past year and provide a preview of upcoming changes. There will be time for attendees to ask questions., ,Stuart Klugman, ,Society of Actuaries, , ," Education Updates from CIA, CAS, and SOA"
8/1/2023,F,3:20 PM,3:20 PM,3:50 PM,Aliber 102,2,Parallel Session,Statistical and Machine Learning III,Hong Li,Hong Li,FALSE,Chengguo Weng,Two-Phase Selection of Representative Contracts for Valuation of Large Variable Annuity Portfolios," A computationally appealing methodology for the valuation of large variable annuities portfolios is a metamodelling framework that evaluates a small set of representative contracts, fits a predictive model based on these computed values, and then extrapolates the model to estimate the values of the remaining contracts. This paper proposes a new two-phase procedure for selecting representative contracts. The representatives from the first phase are determined using contract attributes as in existing metamodelling approaches, but those in the second phase are chosen by utilizing the information contained in the values of the representatives from the first phase. Two numerical studies confirm that our two-phase selection procedure improves upon conventional approaches from the existing literature.","A computationally appealing methodology for the valuation of large variable annuities portfolios is a metamodelling framework that evaluates a small set of representative contracts, fits a predictive model based on these computed values, and then extrapolates the model to estimate the values of the remaining contracts. This paper proposes a new two-phase procedure for selecting representative contracts. The representatives from the first phase are determined using contract attributes as in existing metamodelling approaches, but those in the second phase are chosen by utilizing the information contained in the values of the representatives from the first phase. Two numerical studies confirm that our two-phase selection procedure improves upon conventional approaches from the existing literature.", ,Chengguo Weng, ,University of Waterloo,,,Statistical and machine learning III
8/1/2023,F,3:20 PM,3:50 PM,4:20 PM,Aliber 102,2,Parallel Session,Statistical and Machine Learning III,Hong Li,Hong Li,FALSE,Wenjun Zhu,Machine Learning in Long-Term Mortality Forecasting," We propose a new machine learning-based framework for long-term mortality forecasting. Based on ideas of neighbouring prediction, model ensembling, and tree boosting, this framework can significantly improve the prediction accuracy of long-term mortality. In addition, the proposed framework addresses the challenge of a shrinking pattern in long-term forecasting with information from neighbouring ages and cohorts. An extensive empirical analysis is conducted using various countries and regions in the Human Mortality Database. Results show that this framework reduces the mean absolute percentage error (MAPE) of the 20-year forecasting by almost 50% compared to classic stochastic mortality models, and it also outperforms deep learning-based benchmarks. Moreover, including mortality data from multiple populations can further enhance the long-term prediction performance of this framework.  ","We propose a new machine learning-based framework for long-term mortality forecasting. Based on ideas of neighbouring prediction, model ensembling, and tree boosting, this framework can significantly improve the prediction accuracy of long-term mortality. In addition, the proposed framework addresses the challenge of a shrinking pattern in long-term forecasting with information from neighbouring ages and cohorts. An extensive empirical analysis is conducted using various countries and regions in the Human Mortality Database. Results show that this framework reduces the mean absolute percentage error (MAPE) of the 20-year forecasting by almost 50% compared to classic stochastic mortality models, and it also outperforms deep learning-based benchmarks. Moreover, including mortality data from multiple populations can further enhance the long-term prediction performance of this framework.", ,Wenjun Zhu, ,Nanyang Technological University,,,Statistical and machine learning II
8/1/2023,F,3:20 PM,4:20 PM,4:50 PM,Aliber 102,2,Parallel Session,Statistical and Machine Learning III,Hong Li,Hong Li,FALSE,Hong Li,Pricing Catastrophe Bonds—A Probabilistic Machine Learning Approach,"This paper applies a probabilistic machine learning method to price catastrophe bonds in the primary market. The proposed method combines gradient boosting with conformal predictions and is compared to traditional methods such as linear regression. Using all CAT bonds issued between January 1999 and March 2021, the proposed method is found to be more robust and yields more accurate predictions of the bond spreads. Reasonable confidence intervals are generated by the proposed machine learning method. Additionally, the machine learning methods identify nonlinear relationships between risk factors and bond spreads, suggesting that linear regression could mis-estimate the bond spreads. Overall, this paper demonstrates the potential of machine learning methods in improving the pricing of catastrophe bonds in the primary market.","This paper applies a probabilistic machine learning method to price catastrophe bonds in the primary market. The proposed method combines gradient boosting with conformal predictions and is compared to traditional methods such as linear regression. Using all CAT bonds issued between January 1999 and March 2021, the proposed method is found to be more robust and yields more accurate predictions of the bond spreads. Reasonable confidence intervals are generated by the proposed machine learning method. Additionally, the machine learning methods identify nonlinear relationships between risk factors and bond spreads, suggesting that linear regression could mis-estimate the bond spreads. Overall, this paper demonstrates the potential of machine learning methods in improving the pricing of catastrophe bonds in the primary market.", ,Hong Li, ,University of Guelph,,,Statistical and machine learning II
8/1/2023,F,3:20 PM,3:20 PM,3:50 PM,Aliber 103,3,Parallel Session,Cyber Risk,Maochao Xu,Maochao Xu,TRUE,Xingyun (Claire) Tan,Analysing Reporting Patterns and Frequency of Data Breaches Published by State Attorneys General in the United States,"We study data breach claim notifications and investigates claim frequency by state and severity of the breach in the United States. We identify how breaches are notified over time, and discuss how to project breaches that have incurred but have not been reported yet. We utilise a novel set of public data provided by state Attorneys General that contains dates of occurrence and recent breaches, which are not included in the most widely used public dataset provided by the Privacy Rights Clearinghouse. We introduce this novel data set and compare it with the PRC dataset for cyber researchers to understand their disparities and better use these resources to obtain cyber insurance insights. We implement a new event definition that provides insight into the true impact of data breaches and allows for managing cyber insurance at portfolio level. Our analysis provides important insights. The reporting patterns vary significantly across different time periods and breach sizes, and the average delay between occurrence and reporting has increased across states. The data breach frequency is relatively stable before 2020 but increases subsequently across states. Although the reporting patterns vary across states, states are strongly positively correlated in terms of frequency trends, growth rates, the timing of change in reporting patterns, and trends in the average delay.","We study data breach claim notifications and investigates claim frequency by state and severity of the breach in the United States. We identify how breaches are notified over time, and discuss how to project breaches that have incurred but have not been reported yet. We utilise a novel set of public data provided by state Attorneys General that contains dates of occurrence and recent breaches, which are not included in the most widely used public dataset provided by the Privacy Rights Clearinghouse. We introduce this novel data set and compare it with the PRC dataset for cyber researchers to understand their disparities and better use these resources to obtain cyber insurance insights. We implement a new event definition that provides insight into the true impact of data breaches and allows for managing cyber insurance at portfolio level. Our analysis provides important insights. The reporting patterns vary significantly across different time periods and breach sizes, and the average delay between occurrence and reporting has increased across states. The data breach frequency is relatively stable before 2020 but increases subsequently across states. Although the reporting patterns vary across states, states are strongly positively correlated in terms of frequency trends, growth rates, the timing of change in reporting patterns, and trends in the average delay.",,Xingyun (Claire) Tan, ,University of Melbourne,,,Cyber Risk
8/1/2023,F,3:20 PM,3:50 PM,4:20 PM,Aliber 103,3,Parallel Session,Cyber Risk,Maochao Xu,Maochao Xu,TRUE,Changyue Hu,NLP-Powered Repository and Search Engine for Academic Papers: A Case Study on Cyber Risk Literature With CyLit,"As the body of academic literature continues to grow, researchers face increasing difficulties in effectively searching for relevant resources. Existing databases and search engines often fall short of providing a comprehensive and contextually relevant collection of academic literature. To address this issue, we propose a novel framework that leverages Natural Language Processing (NLP) techniques. This framework automates the retrieval, summarization, and classification of academic literature within a specific research domain. To demonstrate the effectiveness of our approach, we introduce CyLit, an NLP-powered repository specifically designed for the cyber risk literature. CyLit empowers researchers by providing access to context-specific resources and enabling the tracking of trends in the dynamic and rapidly evolving field of cyber risk. Through the automatic processing of large volumes of data, our NLP-powered solution significantly enhances the efficiency and specificity of academic literature searches. By utilizing NLP techniques, we aim to revolutionize the way researchers discover, analyze, and utilize academic resources, ultimately fostering advancements in various domains of knowledge.","As the body of academic literature continues to grow, researchers face increasing difficulties in effectively searching for relevant resources. Existing databases and search engines often fall short of providing a comprehensive and contextually relevant collection of academic literature. To address this issue, we propose a novel framework that leverages Natural Language Processing (NLP) techniques. This framework automates the retrieval, summarization, and classification of academic literature within a specific research domain. To demonstrate the effectiveness of our approach, we introduce CyLit, an NLP-powered repository specifically designed for the cyber risk literature. CyLit empowers researchers by providing access to context-specific resources and enabling the tracking of trends in the dynamic and rapidly evolving field of cyber risk. Through the automatic processing of large volumes of data, our NLP-powered solution significantly enhances the efficiency and specificity of academic literature searches. By utilizing NLP techniques, we aim to revolutionize the way researchers discover, analyze, and utilize academic resources, ultimately fostering advancements in various domains of knowledge.",,Changyue Hu, ,University of Illinois Urbana-Champaign,,,Cyber Risk
8/1/2023,F,3:20 PM,4:20 PM,4:50 PM,Aliber 103,3,Parallel Session,Cyber Risk,Maochao Xu,Maochao Xu,TRUE,Maochao Xu,EBiCop: Ensemble Bivariate Copulas for Modeling Multivariate Cyber Data Breach Risks With Insurance Applications,"Modeling the multivariate dependence among cyber data breach risks presents a significant challenge due to the sparsity and heavy tail properties exhibited by breach events. In this study, we propose a novel ensemble learning approach that effectively captures both the temporal and cross-sectional dependence inherent in cyber risks. Our approach leverages bivariate copulas to generate predictive members, and the resulting predictive distribution is carefully calibrated by minimizing the distribution score. Moreover, we demonstrate the applicability of our proposed model in the domain of insurance pricing. Through extensive simulations and analysis of real-world data, our findings reveal that our approach outperforms existing methodologies reported in the literature. The superior performance of our approach highlights its potential to enhance risk assessment and insurance pricing practices related to cyber data breaches.","Modeling the multivariate dependence among cyber data breach risks presents a significant challenge due to the sparsity and heavy tail properties exhibited by breach events. In this study, we propose a novel ensemble learning approach that effectively captures both the temporal and cross-sectional dependence inherent in cyber risks. Our approach leverages bivariate copulas to generate predictive members, and the resulting predictive distribution is carefully calibrated by minimizing the distribution score. Moreover, we demonstrate the applicability of our proposed model in the domain of insurance pricing. Through extensive simulations and analysis of real-world data, our findings reveal that our approach outperforms existing methodologies reported in the literature. The superior performance of our approach highlights its potential to enhance risk assessment and insurance pricing practices related to cyber data breaches.",,Maochao Xu, ,Illinois State University,,,Cyber Risk
8/1/2023,F,3:20 PM,3:20 PM,3:50 PM,Aliber 107,4,Parallel Session,Risk Modeling and Measurement III,Enrique Thomann,Enrique Thomann,TRUE,Mohammed Adjieteh,Quantile Generalized Least Squares for Robust-Efficient Fitting of Loss Models," Statisticians, financial risk managers, and actuaries rely on statistical models for a variety of purposes, including predicting future losses for pricing, setting adequate reserves, making predictions about future events or outcomes based on past data. However, in the process of modeling, one of the main statistical challenge is that these loss models are indexed by unknown parameters that need to be estimated. Estimating the parameters of these statistical distributions using classical methods such as maximum likelihood and methods of moments may not always yield reliable results, as these methods are not robust and are easily
affected by outliers. This leads to misleading inferences as their estimates may not be accurate. The main goal of this paper is to address the issue of estimating unknown parameters in loss models using the Quantile Generalized Least Squares method. This method is highly efficiency, robust, provides unique estimates, and is computationally simple. We will illustrate this by using distributions from the log-location-scale families.","Statisticians, financial risk managers, and actuaries rely on statistical models for a variety of purposes, including predicting future losses for pricing, setting adequate reserves, making predictions about future events or outcomes based on past data. However, in the process of modeling, one of the main statistical challenge is that these loss models are indexed by unknown parameters that need to be estimated. Estimating the parameters of these statistical distributions using classical methods such as maximum likelihood and methods of moments may not always yield reliable results, as these methods are not robust and are easily
affected by outliers. This leads to misleading inferences as their estimates may not be accurate. The main goal of this paper is to address the issue of estimating unknown parameters in loss models using the Quantile Generalized Least Squares method. This method is highly efficiency, robust, provides unique estimates, and is computationally simple. We will illustrate this by using distributions from the log-location-scale families.", ,Mohammed Adjieteh, ,University of Wisconsin-Milwaukee,,,Risk modeling and measurement II
8/1/2023,F,3:20 PM,3:50 PM,4:20 PM,Aliber 107,4,Parallel Session,Risk Modeling and Measurement III,Enrique Thomann,Enrique Thomann,TRUE,Benjamin Côté,Risk Models Defined on a Family of Arborescent Markovian Random Fields With Poisson Marginals," A new family of tree-based Markov random fields for a vector of discrete random variables will be presented. According to the characteristics of the family, the univariate distribution of the random variables is Poisson and the structure of dependence between them is encrypted in a tree. This family will be used as a basis for building risk models, which will make it possible to integrate the flexibility specific to graphic models into actuarial modelling. Applications in a variety of pratical contexts will be discussed.","A new family of tree-based Markov random fields for a vector of discrete random variables will be presented. According to the characteristics of the family, the univariate distribution of the random variables is Poisson and the structure of dependence between them is encrypted in a tree. This family will be used as a basis for building risk models, which will make it possible to integrate the flexibility specific to graphic models into actuarial modelling. Applications in a variety of pratical contexts will be discussed.", ,Benjamin Côté, ,Université Laval,,,Dependence modeling
8/1/2023,F,3:20 PM,4:20 PM,4:50 PM,Aliber 107,4,Parallel Session,Risk Modeling and Measurement III,Enrique Thomann,Enrique Thomann,TRUE,Enrique Thomann,Ruin With Delayed Claims and Investment," We consider the evolution of the capital of an insurance company that invests in a risky asset and pays claims after a random delay time, extending the analysis of Dassios and Zhao (2013).  Basic properties of the resulting partial integro-differential equation satisfied by the ruin probability are analyzed.  The particular case of deterministic delays can be further studied, resulting in the certainty of ruin for instances in which the volatility of the investment is large.

This is joint work with Dr. Sooie Hoe Lock, Associate Professor at Central Washington University.","We consider the evolution of the capital of an insurance company that invests in a risky asset and pays claims after a random delay time, extending the analysis of Dassios and Zhao (2013). Basic properties of the resulting partial integro-differential equation satisfied by the ruin probability are analyzed. The particular case of deterministic delays can be further studied, resulting in the certainty of ruin for instances in which the volatility of the investment is large.

This is joint work with Dr. Sooie Hoe Lock, Associate Professor at Central Washington University.", ,Enrique Thomann, ,Oregon State University,,,Risk modeling and measurement II
8/1/2023,F,6:00 PM,6:00 PM,7:00 PM,Hotel Fort Des Moines,NA,Reception Prior to Banquet (Shuttle departs from Drake at 5:00 PM and 5:30 PM for hotels and Hotel Fort Des Moines),NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
8/1/2023,F,7:00 PM,7:00 PM,9:15 PM,Hotel Fort Des Moines,NA,Banquet,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
8/2/2023,G,8:30 AM,8:30 AM,9:00 AM,Aliber Hall,NA,Breakfast,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
8/2/2023,G,9:00 AM,9:00 AM,10:30 AM,Aliber 101,1,Parallel Session,(Invited Session) Milliman Special Session,Toby White,Toby White,TRUE,Josh Collins,UNDP-Milliman Global Actuarial Initiative (GAIN)," Program background: In September 2022, the United Nations Development Programme (UNDP)’s Insurance and Risk Financing Facility (IRFF) and Milliman launched the UNDP-Milliman Global Actuarial Initiative (GAIN), which aim to build actuarial profession and expertise in developing countries, helping predict and prepare for risks in these uncertain times.

What is GAIN’s mission? UNDP has identified actuarial capacity and expertise – assessing risk in insurance and finance with mathematical and statistical methods – as a necessary input for the achievement of the Sustainable Development Goals (SDGs). Working in partnership with the UNDP, Milliman will contribute to the growth of actuarial profession and expertise so that governments and the insurance industry can better manage the increasing risks faced by the people and enterprises in developing nations. The scope of this initiative includes:

Building actuarial capacity and expertise of local actuarial professionals.
Enhancing data availability through regulators and the insurance industry.
Supporting countries in adapting with more resilient risk management as part of the climate change impact.
Supporting advocacy to governments, insurers, and others in achieving programme goals.
What are our goals? Through our engagement in many countries through GAIN we have consistently recognized the importance of actuarial science education through local universities as an important pillar of the development of local actuarial talent. The local university is an important step as it provides a more cost-effective approach to education relative to international universities or international actuarial exams, while also retaining the actuarial talent in-country and incentivizing graduates to contribute to the growth of their local profession.

In order to build actuarial capacity, it is important to strengthen the universities with sustainable structures are established to ensure quality actuarial graduates are produced. GAIN has identified several goals to improve actuarial science education in the countries we are engaged in, which are:

Support the development of faculty members and provide access to training materials
Provide access to affordable educational tools and resources to faculty members and students
Increase awareness of the actuarial profession amongst prospective university students
Strengthen the linkage between industry and academia through better access to research opportunities
Join an alliance with the UNDP-Milliman partnership: As a professional firm we recognize our skillsets and expertise at Milliman do not reflect all the academic needs of higher education institutions. Therefore, we invite professionals and academics with the relevant expertise to work with us to achieve our goals. Supporting GAIN allows organizations and individuals to gain exposure through their commitment to the actuarial profession, share your knowledge and learn from peers around the world, and associate themselves with expertise and leadership in advancing the SDGs.","Program background: In September 2022, the United Nations Development Programme (UNDP)’s Insurance and Risk Financing Facility (IRFF) and Milliman launched the UNDP-Milliman Global Actuarial Initiative (GAIN), which aim to build actuarial profession and expertise in developing countries, helping predict and prepare for risks in these uncertain times.

What is GAIN’s mission? UNDP has identified actuarial capacity and expertise – assessing risk in insurance and finance with mathematical and statistical methods – as a necessary input for the achievement of the Sustainable Development Goals (SDGs). Working in partnership with the UNDP, Milliman will contribute to the growth of actuarial profession and expertise so that governments and the insurance industry can better manage the increasing risks faced by the people and enterprises in developing nations. The scope of this initiative includes:

Building actuarial capacity and expertise of local actuarial professionals.
Enhancing data availability through regulators and the insurance industry.
Supporting countries in adapting with more resilient risk management as part of the climate change impact.
Supporting advocacy to governments, insurers, and others in achieving programme goals.
What are our goals? Through our engagement in many countries through GAIN we have consistently recognized the importance of actuarial science education through local universities as an important pillar of the development of local actuarial talent. The local university is an important step as it provides a more cost-effective approach to education relative to international universities or international actuarial exams, while also retaining the actuarial talent in-country and incentivizing graduates to contribute to the growth of their local profession.

In order to build actuarial capacity, it is important to strengthen the universities with sustainable structures are established to ensure quality actuarial graduates are produced. GAIN has identified several goals to improve actuarial science education in the countries we are engaged in, which are:

Support the development of faculty members and provide access to training materials
Provide access to affordable educational tools and resources to faculty members and students
Increase awareness of the actuarial profession amongst prospective university students
Strengthen the linkage between industry and academia through better access to research opportunities
Join an alliance with the UNDP-Milliman partnership: As a professional firm we recognize our skillsets and expertise at Milliman do not reflect all the academic needs of higher education institutions. Therefore, we invite professionals and academics with the relevant expertise to work with us to achieve our goals. Supporting GAIN allows organizations and individuals to gain exposure through their commitment to the actuarial profession, share your knowledge and learn from peers around the world, and associate themselves with expertise and leadership in advancing the SDGs.", ,Josh Collins, ,  Milliman, , , Milliman Special Session
8/2/2023,G,9:00 AM,9:00 AM,9:30 AM,Aliber 102,2,Parallel Session,Statistical and Machine Learning IV,Gee Lee,Gee Lee,TRUE,Chudamani Poudyal,Robust Method of Trimmed and Winsorized Moments for Truncated and Censored Lognormal Severity Distributions," When constructing parametric models to predict the cost of future claims, several important details have to be taken into account: (i) models should be designed to accommodate deductibles, policy limits, and coinsurance factors, (ii) parameters should be estimated robustly to control the 
influence of outliers on model predictions, and (iii) all point predictions should be augmented with estimates of their uncertainty. The methodology proposed in this presentation provides a framework for addressing all these aspects simultaneously. Using payment-per-payment and payment-per-loss variables, we construct the adaptive version of method of winsorized moments (MWM) estimators for the parameters of truncated and censored lognormal distribution. Further, the asymptotic distributional properties of this approach are derived and compared with those of the maximum likelihood 
estimator (MLE) and method of trimmed moments (MTM) estimators. The latter being a primary competitor to MWM. Moreover, the theoretical results are validated with extensive simulation studies and risk measure sensitivity analysis. Finally, practical performance of these methods is illustrated using the well-studied data set of 1500 U.S. indemnity losses. With this real data set, it is also demonstrated that the composite models do not provide much 
improvement in the quality of predictive models compared to a stand-alone fitted distribution specially for truncated and censored sample data.","When constructing parametric models to predict the cost of future claims, several important details have to be taken into account: (i) models should be designed to accommodate deductibles, policy limits, and coinsurance factors, (ii) parameters should be estimated robustly to control the 
influence of outliers on model predictions, and (iii) all point predictions should be augmented with estimates of their uncertainty. The methodology proposed in this presentation provides a framework for addressing all these aspects simultaneously. Using payment-per-payment and payment-per-loss variables, we construct the adaptive version of method of winsorized moments (MWM) estimators for the parameters of truncated and censored lognormal distribution. Further, the asymptotic distributional properties of this approach are derived and compared with those of the maximum likelihood 
estimator (MLE) and method of trimmed moments (MTM) estimators. The latter being a primary competitor to MWM. Moreover, the theoretical results are validated with extensive simulation studies and risk measure sensitivity analysis. Finally, practical performance of these methods is illustrated using the well-studied data set of 1500 U.S. indemnity losses. With this real data set, it is also demonstrated that the composite models do not provide much 
improvement in the quality of predictive models compared to a stand-alone fitted distribution specially for truncated and censored sample data.", ,Chudamani Poudyal, ,University of Wisconsin-Milwaukee,,,Statistical and machine learning IV
8/2/2023,G,9:00 AM,9:30 AM,10:00 AM,Aliber 102,2,Parallel Session,Statistical and Machine Learning IV,Gee Lee,Gee Lee,TRUE,Hong Beng Lim,Discrimination-Free Actuarial Decision-Making via Debiased Data Representations," Insurers have traditionally claimed compliance with nondiscrimination laws by excluding protected group labels, such as gender and race, from use in insurance processes. Such an approach no longer suffices given the increasing deployment of machine learning algorithms, which are increasingly able to infer protected group labels from apparently unrelated data. This has created recent interest among actuarial researchers in methodologies which guarantee fair outcomes for protected groups. Existing such methods require complete information on protected group labels: an untenable requirement for sensitive information such as race or religion. We propose a neural network approach which utilizes adversarial learning to enforce group fairness under partially observed protected group labels. We demonstrate that such networks are a marked improvement in accuracy over methods requiring complete protected group labels, while only a small proportion of observed labels (e.g. 5%) is needed to produce fairness gains not substantially worse than complete information.","Insurers have traditionally claimed compliance with nondiscrimination laws by excluding protected group labels, such as gender and race, from use in insurance processes. Such an approach no longer suffices given the increasing deployment of machine learning algorithms, which are increasingly able to infer protected group labels from apparently unrelated data. This has created recent interest among actuarial researchers in methodologies which guarantee fair outcomes for protected groups. Existing such methods require complete information on protected group labels: an untenable requirement for sensitive information such as race or religion. We propose a neural network approach which utilizes adversarial learning to enforce group fairness under partially observed protected group labels. We demonstrate that such networks are a marked improvement in accuracy over methods requiring complete protected group labels, while only a small proportion of observed labels (e.g. 5%) is needed to produce fairness gains not substantially worse than complete information.", ,Hong Beng Lim, ,Chinese University of Hong Kong,,,Statistical and machine learning IV
8/2/2023,G,9:00 AM,10:00 AM,10:30 AM,Aliber 102,2,Parallel Session,Statistical and Machine Learning IV,Gee Lee,Gee Lee,TRUE,Gee Lee,Nonparametric Intercept Regularization for Insurance Claim Frequency Regression Models," In a subgroup analysis for an actuarial problem, the goal is for the investigator to classify the policyholders into unique groups, where the claims experience within each group are made as homogeneous as possible. In this paper, we illustrate how the alternating direction method of multipliers (ADMM) approach for subgroup analysis can be modified so that it can be more easily incorporated into an insurance claims analysis. We present an approach to penalize adjacent coefficients only, and show how the algorithm can be implemented for fast estimation of the parameters. We present three different cases of the model, depending on the level of dependence among the different coverage groups within the data. In addition, we provide an interpretation of the credibility problem using both random effects and fixed effects, where the fixed effects approach corresponds to the ADMM approach to subgroup analysis, while the random effects approach represents the classic Bayesian approach. In an empirical study, we demonstrate how these approaches can be applied to real data using the Wisconsin Local Government Property Insurance Fund (LGPIF) data. ","In a subgroup analysis for an actuarial problem, the goal is for the investigator to classify the policyholders into unique groups, where the claims experience within each group are made as homogeneous as possible. In this paper, we illustrate how the alternating direction method of multipliers (ADMM) approach for subgroup analysis can be modified so that it can be more easily incorporated into an insurance claims analysis. We present an approach to penalize adjacent coefficients only, and show how the algorithm can be implemented for fast estimation of the parameters. We present three different cases of the model, depending on the level of dependence among the different coverage groups within the data. In addition, we provide an interpretation of the credibility problem using both random effects and fixed effects, where the fixed effects approach corresponds to the ADMM approach to subgroup analysis, while the random effects approach represents the classic Bayesian approach. In an empirical study, we demonstrate how these approaches can be applied to real data using the Wisconsin Local Government Property Insurance Fund (LGPIF) data.", ,Gee Lee, ,Michigan State University,,,Statistical and machine learning IV
8/2/2023,G,9:00 AM,9:00 AM,9:30 AM,Aliber 103,3,Parallel Session,Quantitative Finance II,Patrice Gaillardetz,Patrice Gaillardetz,TRUE,Tsz Hin Ng,Capital-Allocation-Induced Risk Sharing,"The relationship between capital allocation principles and risk-sharing rules is explored. While the former is concerned with allocating capitals to different lines of business based on the relationship between the aggregated risk and individual risks within a corporation, the latter is an ex ante arrangement between a group of participants which share the need to trade risks with each other. Taking advantages of their analogy, we introduce a novel idea of generating risk-sharing rules by randomizing existing capital allocation principles. Such a new approach is complimentary to known risk sharing principles in the previous literature which were largely based on certain economic principles and Pareto-optimality.","The relationship between capital allocation principles and risk-sharing rules is explored. While the former is concerned with allocating capitals to different lines of business based on the relationship between the aggregated risk and individual risks within a corporation, the latter is an ex ante arrangement between a group of participants which share the need to trade risks with each other. Taking advantages of their analogy, we introduce a novel idea of generating risk-sharing rules by randomizing existing capital allocation principles. Such a new approach is complimentary to known risk sharing principles in the previous literature which were largely based on certain economic principles and Pareto-optimality.",,Tsz Hin Ng,,University of Illinois at Urbana-Champaign,,,Quantitative finance II
8/2/2023,G,9:00 AM,9:30 AM,10:00 AM,Aliber 103,3,Parallel Session,Quantitative Finance II,Patrice Gaillardetz,Patrice Gaillardetz,TRUE,Yueman Feng,Multidimensional Pair Trading Strategy in China’s Stock Market," Pair trading is one of the most important statistical arbitrage strategies. It utilizes cointegration tests to identify cointegrated pairs of stocks and apply the mean-reverse strategy to profit from temporary asset mispricing. However, the strategy is limited to two cointegrated assets and is highly restricted in practice. With the extension of multiple cointegration tests to high-dimensional data, we proposed a multidimensional pair trading strategy. We implemented it with Johansen's likelihood ratio test to enhance the performance of the CSI300 index, the most crucial index in China's stock market. The strategy was backtested in 2021, and a 44.17% annual return was achieved with a Sharpe ratio of 1.62. The method outperformed the CSI300 index significantly. Apart from this baseline model, two models with different cointegration tests are also included to fix the over-rejection of Johansen's test and to capture the regime shift of component stocks. The two alternative tests used are the Bykhovskaya-Gorin’s test and the Gregory-Hansen’s cointegration test, respectively. Both models enhanced our baseline strategy's performance but with only slight improvement. Multiple reasons causing the limited enhancement are discussed for future studies.","Pair trading is one of the most important statistical arbitrage strategies. It utilizes cointegration tests to identify cointegrated pairs of stocks and apply the mean-reverse strategy to profit from temporary asset mispricing. However, the strategy is limited to two cointegrated assets and is highly restricted in practice. With the extension of multiple cointegration tests to high-dimensional data, we proposed a multidimensional pair trading strategy. We implemented it with Johansen's likelihood ratio test to enhance the performance of the CSI300 index, the most crucial index in China's stock market. The strategy was backtested in 2021, and a 44.17% annual return was achieved with a Sharpe ratio of 1.62. The method outperformed the CSI300 index significantly. Apart from this baseline model, two models with different cointegration tests are also included to fix the over-rejection of Johansen's test and to capture the regime shift of component stocks. The two alternative tests used are the Bykhovskaya-Gorin’s test and the Gregory-Hansen’s cointegration test, respectively. Both models enhanced our baseline strategy's performance but with only slight improvement. Multiple reasons causing the limited enhancement are discussed for future studies.", ,Yueman Feng, ,The University of Hong Kong,,,Quantitative finance II
8/2/2023,G,9:00 AM,10:00 AM,10:30 AM,Aliber 103,3,Parallel Session,Quantitative Finance II,Patrice Gaillardetz,Patrice Gaillardetz,TRUE,Patrice Gaillardetz,Robust Risk Minimizing Hedging Strategies," We consider the evaluation of financial derivatives using robust hedging strategies. Robust technic applications to finance and insurance have recently gained popularity due to their ability in mitigating model risk. Model risk arises when strategies (or models) become in and out of sync with the market. A model is considered robust if it can adapt to a wide range of market-dependent factors. However, robust techniques can be costly and computationally demanding, especially for complex financial and insurance products. To address these challenges, we propose semi-robust strategies that are both model-independent and cost-effective for contingent claims. We explore models that minimize risk exposure locally and globally using constrained robust optimization based on asymmetric norms. The norms primarily measure the size, magnitude, or distance of the financial position from the expected hedging portfolio. Hedging strategies are solved using backward stochastic dynamic programming that we imbed with parametric technics to speed up the running time and cope with the curse of dimensionality in dynamic programming. To demonstrate the flexibility of our strategies, we present numerical examples featuring European call options.","We consider the evaluation of financial derivatives using robust hedging strategies. Robust technic applications to finance and insurance have recently gained popularity due to their ability in mitigating model risk. Model risk arises when strategies (or models) become in and out of sync with the market. A model is considered robust if it can adapt to a wide range of market-dependent factors. However, robust techniques can be costly and computationally demanding, especially for complex financial and insurance products. To address these challenges, we propose semi-robust strategies that are both model-independent and cost-effective for contingent claims. We explore models that minimize risk exposure locally and globally using constrained robust optimization based on asymmetric norms. The norms primarily measure the size, magnitude, or distance of the financial position from the expected hedging portfolio. Hedging strategies are solved using backward stochastic dynamic programming that we imbed with parametric technics to speed up the running time and cope with the curse of dimensionality in dynamic programming. To demonstrate the flexibility of our strategies, we present numerical examples featuring European call options.", ,Patrice Gaillardetz, ,Concordia University,,,Quantitative finance II
8/2/2023,G,9:00 AM,9:00 AM,9:30 AM,Aliber 107,4,Parallel Session,Insurance Economics II,Doug Bujakowski,Doug Bujakowski,TRUE,Youngsun Kim,Persistence of Insurance Redlining: Exploring Distribution of Insurance Agencies and Implications," The availability of insurance and the issue of discrimination have long been significant problems within the insurance industry. Access to insurance is crucial for underrepresented communities as it provides financial protection and promotes economic stability for both the community and its residents. In this paper, we address these concerns by revisiting the seminal study conducted by Squires et al. (1991) and examine the persistence of insurance redlining in the Milwaukee metropolitan area in Wisconsin. Utilizing recent census data, we investigate the relationship between the locations of insurance agencies and the racial composition of neighborhoods. Furthermore, we explore potential variations in these patterns between independent and exclusive agencies to determine if there are differing implications.","The availability of insurance and the issue of discrimination have long been significant problems within the insurance industry. Access to insurance is crucial for underrepresented communities as it provides financial protection and promotes economic stability for both the community and its residents. In this paper, we address these concerns by revisiting the seminal study conducted by Squires et al. (1991) and examine the persistence of insurance redlining in the Milwaukee metropolitan area in Wisconsin. Utilizing recent census data, we investigate the relationship between the locations of insurance agencies and the racial composition of neighborhoods. Furthermore, we explore potential variations in these patterns between independent and exclusive agencies to determine if there are differing implications.", ,Youngsun Kim, ,University of Wisconsin-Madison,,,Insurance economics II
8/2/2023,G,9:00 AM,9:30 AM,10:00 AM,Aliber 107,4,Parallel Session,Insurance Economics II,Doug Bujakowski,Doug Bujakowski,TRUE,Zexing Hu,Evaluation of the New NAIC Economic Scenario Generator," An Economic Scenario Generator (ESG) is employed to simulate data pertaining to the economy to help insurance regulators in establishing policy. In 2022, the National Association of Insurance Commissioners released a new ESG model (NAIC) and is considering replacing the previously used Academy's Interest Rate Generator (AIRG). To evaluate the two models, we first establish a set of acceptance criteria by employing standard econometric modeling techniques and researching key historical periods. Then, we compare and assess the performance of the two ESGs based on the criteria we developed, in order to discuss the strengths and weaknesses of the two models. Our findings indicate that the NAIC model produces more volatile results than the AIRG model.","An Economic Scenario Generator (ESG) is employed to simulate data pertaining to the economy to help insurance regulators in establishing policy. In 2022, the National Association of Insurance Commissioners released a new ESG model (NAIC) and is considering replacing the previously used Academy's Interest Rate Generator (AIRG). To evaluate the two models, we first establish a set of acceptance criteria by employing standard econometric modeling techniques and researching key historical periods. Then, we compare and assess the performance of the two ESGs based on the criteria we developed, in order to discuss the strengths and weaknesses of the two models. Our findings indicate that the NAIC model produces more volatile results than the AIRG model.", ,Zexing Hu, ,UCSB,,,Insurance economics II
8/2/2023,G,9:00 AM,10:00 AM,10:30 AM,Aliber 107,4,Parallel Session,Insurance Economics II,Doug Bujakowski,Doug Bujakowski,TRUE,Doug Bujakowski,The Liability Insurance Consumption Spiral: Evidence From Chinese Cities,"Liability insurance is a vital source of financial protection and risk management for individuals and organizations. However, its widespread adoption carries certain unintended consequences, including the amplification of liability risks for uninsured and underinsured populations. This may result in a liability insurance consumption spiral, where purchases by some incentivize others to follow suit. The current study provides the first empirical tests of the consumption spiral hypothesis. Using data from 280 Chinese cities over an eight-year period (2011-2018), we find evidence that liability insurance purchases influence those in nearby geographic units and time periods. These knock-on effects are substantial and support the notion that purchase decisions are positively correlated. Additionally, our results reveal a key externality of liability insurance markets and the ways in which consumption diffuses over time and space.","Liability insurance is a vital source of financial protection and risk management for individuals and organizations. However, its widespread adoption carries certain unintended consequences, including the amplification of liability risks for uninsured and underinsured populations. This may result in a liability insurance consumption spiral, where purchases by some incentivize others to follow suit. The current study provides the first empirical tests of the consumption spiral hypothesis. Using data from 280 Chinese cities over an eight-year period (2011-2018), we find evidence that liability insurance purchases influence those in nearby geographic units and time periods. These knock-on effects are substantial and support the notion that purchase decisions are positively correlated. Additionally, our results reveal a key externality of liability insurance markets and the ways in which consumption diffuses over time and space.", ,Doug Bujakowski, ,Drake University,,,Insurance economics II
8/2/2023,G,9:00 AM,9:00 AM,9:30 AM,Aliber 108,5,Parallel Session,Education and Professional Development II,Stefanos Orfanos,Stefanos Orfanos,TRUE,Mike Smith and Cristina Mano,Tools for Counting Actuaries: Actuarial Density and Actuarial Penetration,"In recent presentations, the International Actuarial Association (IAA) states that there are more than 75,000 actuaries in the world. Given a world population of 7.9 billion persons, the actuarial density (number of actuaries divided by population) is greater than 9.5 per million persons. The IAA also provided the researchers a database of recent and historical data on the number of actuaries for each Full Member Association (FMA) over a twenty-year period. 

After adjustments for double counting due to multiple associations in a country and multiple memberships by an individual, we estimate that the current global actuarial density for FMAs is roughly 15 actuaries per million persons The FMA data are primarily from more developed countries, thus in part explaining the difference in the two estimates of actuarial density. In twenty years, actuarial density has increased two and a half times. 

The supply and demand for actuaries is in a readjustment period: there are ever-increasing actuarial requirements from insurers, regulators, and superintendents (e.g., IFRS 17) along with expansion by adventurous and entrepreneurial actuaries into non-traditional areas. Challenges to actuarial growth come from increased automation, artificial intelligence, machine learning etc. combined with the entry of non-actuarial experts into areas traditionally reserved for actuaries. 

In the current project, we applied panel data modeling techniques to a database that includes 61 countries to explore some of the economic and demographic factors that affect the number of actuaries across nations and over time. 

We predict that, ceteris paribus, the number of actuaries will increase and certainly vary significantly from country to country. Relating the number of actuaries in a country to such items as population, gross domestic product, insurance premium, and other observable variables can give insight into the development of a robust model. The concepts of actuarial density and actuarial penetration (the number of actuaries divided by an economic measure such as GDP or insurance premium) can be useful to illustrate and explain the volatility over time. ","In recent presentations, the International Actuarial Association (IAA) states that there are more than 75,000 actuaries in the world. Given a world population of 7.9 billion persons, the actuarial density (number of actuaries divided by population) is greater than 9.5 per million persons. The IAA also provided the researchers a database of recent and historical data on the number of actuaries for each Full Member Association (FMA) over a twenty-year period. 

After adjustments for double counting due to multiple associations in a country and multiple memberships by an individual, we estimate that the current global actuarial density for FMAs is roughly 15 actuaries per million persons The FMA data are primarily from more developed countries, thus in part explaining the difference in the two estimates of actuarial density. In twenty years, actuarial density has increased two and a half times. 

The supply and demand for actuaries is in a readjustment period: there are ever-increasing actuarial requirements from insurers, regulators, and superintendents (e.g., IFRS 17) along with expansion by adventurous and entrepreneurial actuaries into non-traditional areas. Challenges to actuarial growth come from increased automation, artificial intelligence, machine learning etc. combined with the entry of non-actuarial experts into areas traditionally reserved for actuaries. 

In the current project, we applied panel data modeling techniques to a database that includes 61 countries to explore some of the economic and demographic factors that affect the number of actuaries across nations and over time. 

We predict that, ceteris paribus, the number of actuaries will increase and certainly vary significantly from country to country. Relating the number of actuaries in a country to such items as population, gross domestic product, insurance premium, and other observable variables can give insight into the development of a robust model. The concepts of actuarial density and actuarial penetration (the number of actuaries divided by an economic measure such as GDP or insurance premium) can be useful to illustrate and explain the volatility over time.",,Mike Smith,Cristina Mano,Caribbean Actuarial Association,State University of Rio de Janeiro,,Education and Professional Development II
8/2/2023,G,9:00 AM,9:30 AM,10:00 AM,Aliber 108,5,Parallel Session,Education and Professional Development II,Stefanos Orfanos,Stefanos Orfanos,TRUE,Lusani Mulaudzi,Analysis of Throughput Rates in a South African University Actuarial Program.,"This paper presents the results of an investigation of throughput rates of students studying actuarial science at a South African public university. This is done as part of a master's in philosophy dissertation that is investigating the costs and benefits of an academic development program amongst actuarial students. 
The analysis includes the use of descriptive statistics, statistical tests and Bayesian networks. The study aims to identify the key factors for success and to determine a suitable model for future throughput rates.  ","This paper presents the results of an investigation of throughput rates of students studying actuarial science at a South African public university. This is done as part of a master's in philosophy dissertation that is investigating the costs and benefits of an academic development program amongst actuarial students. 
The analysis includes the use of descriptive statistics, statistical tests and Bayesian networks. The study aims to identify the key factors for success and to determine a suitable model for future throughput rates.",,Lusani Mulaudzi, ,University of Cape Town,,,Education and Professional Development II
8/2/2023,G,9:00 AM,10:00 AM,10:30 AM,Aliber 108,5,Parallel Session,Education and Professional Development II,Stefanos Orfanos,Stefanos Orfanos,TRUE,Stefanos Orfanos,AI in Actuarial Education and Practice,"This presentation is about the impact of AI on actuarial education and practice. We begin with a discussion of Large Language Models (LLMs) and ChatGPT, highlighting their neural network structures and training processes. We then explore the potential uses of ChatGPT in academic and professional settings, and identify ways it may influence the actuarial education and the future development of actuarial credentialing in the United States. More specifically, we examine how LLMs, such as ChatGPT, can be utilized as cheat codes, tutors, learning partners, teaching tools, virtual graders, and assistants. We also touch upon the implications of LLMs for actuarial practice, including the role of technology in shaping the profession and its ethical dimensions.","This presentation is about the impact of AI on actuarial education and practice. We begin with a discussion of Large Language Models (LLMs) and ChatGPT, highlighting their neural network structures and training processes. We then explore the potential uses of ChatGPT in academic and professional settings, and identify ways it may influence the actuarial education and the future development of actuarial credentialing in the United States. More specifically, we examine how LLMs, such as ChatGPT, can be utilized as cheat codes, tutors, learning partners, teaching tools, virtual graders, and assistants. We also touch upon the implications of LLMs for actuarial practice, including the role of technology in shaping the profession and its ethical dimensions.",,Stefanos Orfanos, ,Georgia State University,,,Education and Professional Development II
8/2/2023,G,9:00 AM,9:00 AM,9:30 AM,Aliber 112,6,Parallel Session,Mortality and Longevity Modeling II,Hongjuan Zhou,Hongjuan Zhou,TRUE,Yechao Meng,Enhancing Mortality Prediction via Selection of Age Bands," Similarities of age-specific mortality patterns or trends among ages have been widely observed in real-world data and are believed to be conducive to improving future mortality predicting accuracy. We provide insights on detecting similarities and borrowing information among age-specific mortality patterns by proposing a novel predicting framework which formulates mortality forecasts with enhanced predicting accuracy. The framework decomposes the overall predicting goal into multiple individual tasks and searches for in- dividual age band to ensure the mortality prediction of each target age can receive the benefit of borrowing information across ages to the largest extent. We further consider extending the idea of borrowing information among ages to multi-population scenarios with a more comprehensive framework that can take the information among different ages and across different populations into consideration simultaneously and proposed three different approaches: a distance-based approach, an ensemble-based approach, and an ACF model-based approach. Extensive numerical studies with the Human Mortality Database (HMD) confirm an overall improvement in predicting the accuracy of all proposed methods for the majority of ages, especially for adults and retiree groups. Additionally, several general stylized facts of how ages from multiple populations are borrowed by the distance-based method are provided.","Similarities of age-specific mortality patterns or trends among ages have been widely observed in real-world data and are believed to be conducive to improving future mortality predicting accuracy. We provide insights on detecting similarities and borrowing information among age-specific mortality patterns by proposing a novel predicting framework which formulates mortality forecasts with enhanced predicting accuracy. The framework decomposes the overall predicting goal into multiple individual tasks and searches for in- dividual age band to ensure the mortality prediction of each target age can receive the benefit of borrowing information across ages to the largest extent. We further consider extending the idea of borrowing information among ages to multi-population scenarios with a more comprehensive framework that can take the information among different ages and across different populations into consideration simultaneously and proposed three different approaches: a distance-based approach, an ensemble-based approach, and an ACF model-based approach. Extensive numerical studies with the Human Mortality Database (HMD) confirm an overall improvement in predicting the accuracy of all proposed methods for the majority of ages, especially for adults and retiree groups. Additionally, several general stylized facts of how ages from multiple populations are borrowed by the distance-based method are provided.", ,Yechao Meng, ,University of Prince Edward Island,,,Mortality and longevity modeling I
8/2/2023,G,9:00 AM,9:30 AM,10:00 AM,Aliber 112,6,Parallel Session,Mortality and Longevity Modeling II,Hongjuan Zhou,Hongjuan Zhou,TRUE,Hongjuan Zhou,Modeling and Pricing Long COVID Mortality Risk by Mixed Fractional Brownian Motion,"Recently, the impact of COVID-19 pandemic on mortality risk has been widely studied. Yet the excess mortality was observed in the past few years, researchers are also interested to examine the long COVID on the mortality and the corresponding mortality-linked securities. This question can be well structured and answered using long-range dependence processes, which is as a natural choice to consider the stochastic mortality dynamics based on the mortality data that includes pandemic. Additionally, as pointed out in the recent studies, including by Li et al [2022], the interest rate and mortality rate are found to be correlated due to the effect of COVID-19 pandemic. We will apply a two-dimensional stochastic model driven by mixed fractional Brownian motions to model the dynamics of mortality and interest rate while allowing their correlated diffusions. As an application, we will consider the mortality pricing problem for both longevity bond and catastrophic mortality bond, and quantify how the phenomenon of long COVID can affect the mortality risk in the capital market.","Recently, the impact of COVID-19 pandemic on mortality risk has been widely studied. Yet the excess mortality was observed in the past few years, researchers are also interested to examine the long COVID on the mortality and the corresponding mortality-linked securities. This question can be well structured and answered using long-range dependence processes, which is as a natural choice to consider the stochastic mortality dynamics based on the mortality data that includes pandemic. Additionally, as pointed out in the recent studies, including by Li et al [2022], the interest rate and mortality rate are found to be correlated due to the effect of COVID-19 pandemic. We will apply a two-dimensional stochastic model driven by mixed fractional Brownian motions to model the dynamics of mortality and interest rate while allowing their correlated diffusions. As an application, we will consider the mortality pricing problem for both longevity bond and catastrophic mortality bond, and quantify how the phenomenon of long COVID can affect the mortality risk in the capital market.",,Hongjuan Zhou, ,Arizona State University,,,Mortality and longevity modeling I
8/2/2023,G,10:30 AM,10:30 AM,11:00 AM,Aliber Hall,NA,Coffee Break,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
8/2/2023,G,11:00 AM,11:00 AM,12:00 PM,Aliber 101,7,Keynote Session,NA,Margie Rosenberg,Margie Rosenberg,TRUE,Edward (Jed) Frees,Diversifying Insurable Risk Portfolios,"Commercial organizations face many risks such as damage to their buildings due to fire, liability resulting from management misbehavior, and threats external to the organization such as cyber attacks. Firms have financial responsibility for these risks, they ‘own’ them, and so it is natural to refer to the collection of these many risks as a portfolio. Analogous to the familiar asset portfolio allocation methods, this talk demonstrates how risk managers can use constrained optimization algorithms to provide guidance on the amount of risk to retain or transfer. Using a case study, we are largely able to corroborate the work of expert risk brokers who make risk retention decisions using their extensive experience and knowledge of the worldwide insurance marketplace. Moreover, this insurance framework supplements their work by emphasizing the risk versus return/cost trade-off and offering data visualization educational tools that help improve managers’ financial literacy.","Commercial organizations face many risks such as damage to their buildings due to fire, liability resulting from management misbehavior, and threats external to the organization such as cyber attacks. Firms have financial responsibility for these risks, they ‘own’ them, and so it is natural to refer to the collection of these many risks as a portfolio. Analogous to the familiar asset portfolio allocation methods, this talk demonstrates how risk managers can use constrained optimization algorithms to provide guidance on the amount of risk to retain or transfer. Using a case study, we are largely able to corroborate the work of expert risk brokers who make risk retention decisions using their extensive experience and knowledge of the worldwide insurance marketplace. Moreover, this insurance framework supplements their work by emphasizing the risk versus return/cost trade-off and offering data visualization educational tools that help improve managers’ financial literacy.",,Edward (Jed) Frees,NA,"University of Wisconsin – Madison, Australian National University",NA,NA,NA
8/2/2023,G,12:00 PM,12:00 PM,12:20 PM,Aliber 101,9,Closing Remarks,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA
